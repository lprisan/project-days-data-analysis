---
title: "HIK Project Day -- Linnaruum: Day 1 (11.10.2017)"
subtitle: "Preliminary Research Report"
author: "Luis P. Prieto, María Jesús Rodríguez-Triana, Shashi Kant Shankar and Terje Väljataga"
abstract: "This document describes the research questions and data gathered for the first session of the HIK project days with activities outside the classroom (dubbed 'Linnaruum'), that took place on the 11 October 2017. It also explores what kinds of questions can be answered with such data (currently under analysis), so that the whole (learning and research) process can be improved for future iterations of the event."
bibliography: project-days.bib
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, error = F)

library(ggplot2)
library(data.table)
library(xlsx)
library(reshape2)
library(FactoMineR)
library(factoextra)
library(GGally)

source("../src/preprocessMultimodalTracker.R")

```

As part of its activities to promote educational innovation among local schools, TLU's Center for Innovation in Education (HIK, in Estonian) organizes a series of 'project days', learning experiences for students at different levels of primary/secondary education, that are inter-disciplinary and project-oriented. These activities are usually designed and enacted by HIK's didactics teachers, and offered to classes of local schools multiple times throughout the academic course.

This report is part of a joint collaboration between HIK and the TLU's CEITER project to turn these project days into the subject of a co-created research and innovation process. More concretely, it reports about the first pilot of these events that took place on October 11th, 2017. This project day had as a topic the exploration of a concrete urban space (Tallinn's Kadriorg neighborhood), and included both classroom and outdoor activities (in the neighborhood itself).

The goal of this report is to serve as a basis for **discussions about how to best organize the next iterations of this project day, both from the learner experience** (e.g., improving the activities or resources to be used) **and from the research** (e.g., new research questions or data sources needed) **perspectives**. 

In the following sections, we summarize briefly: a) the initial set of research questions posed; b) the methodology used to collect and analyze data (still in progress); c) what happened on the day of the event, what data has been gathered, what kind of questions are being explored in the analysis, and what kind of responses we can get from the current data; and finally, d) poses open questions about how the whole process can be improved.

# Research questions

Although in the preparatory meetings between HIK and CEITER researchers there were no clear, concrete research question expressed, several topics were mentioned as interesting and worthy of exploration, especially around two areas:

* **Collaborative problem solving (CPS)**, which often includes issues such as participation, role distribution or role-taking, etc.
* **Self-regulated learning**, including the ability to follow instructions autonomously, reflecting on their own learning strategies, engagement and flow, etc.
* ... and, of course, there were also mentions of finding out whether the activities were successful in teaching something to the students, and were considered relevant for that purpose (i.e., more **content-related learning**)

# Methodology

Given the exploratory nature of the study at this stage, and the complexity of the phenomena of interest (mentioned in the last section), we chose a *mixed method approach* [@creswell2013research], combining both qualitative and quantitative data gathering and analysis. This combination of data sources, and the inclusion of sensors to understand the physical space around the activities, makes this research part of the *multimodal learning analytics* field [@blikstein2013multimodal].

## Data gathering

The following techniques were used to gather data about the learners and the learning activities, at different levels:

* At the individual student level:
    * **An observer** followed each of the groups of students, noting down every 5 minutes *what kind of engagement* the student showed towards solving the tasks (see the observer form [here](https://docs.google.com/forms/d/e/1FAIpQLSdXZUIjWPwxfV-Llm_XCy5y3OPEEE270E0s7xPWQEbADydaRg/viewform)) . Given that many of the observers were not Estonian-speaking, we chose an observation schema based on the observable physical engagement of each student (similar to the one used by @spikol2017using).
    * After performing each activity, the students themselves answered a **questionnaire** about the activity, its difficulty and relevance, as well as their perceived individual contribution in the collaboration process (see the form [here](https://docs.google.com/forms/d/e/1FAIpQLSf4CoYnq1acQb7f1nF7PCajSvLnMfKCQhVSYQqx2eHqOkH3Og/viewform))
    * After all the activities, students answered **a few more questions** about their overall impression of the activities, and a short reflection about their problem solving strategies and role-taking (see the form  [here](https://docs.google.com/forms/d/e/1FAIpQLScHyFD1p2WBqVk5FDiyVkk-LdpVucYs1oDtpx9-vJqSbGIdOA/viewform))

* At the group-of-students level:
    * **The observers** provided, along with the detailed observations about each individual, more general open comments about how the group solved the tasks (every 5 minues)
    * The observer was also equipped with a **tablet with tracking software**, aimed to record the (approximate) movement of the groups as they solved the activities. This includes the *amount of movement* (e.g., to distinguish walking, running vs. more static positions), *GPS data* (e.g., to see the approximate path around the neighborhood) as well as *beacon data* (to distinguish in which rooms the groups were over time)
    * The **outcomes** of the learning activities (e.g., the responses of the groups for each task) were supposed to be introduced in the activity worksheet in Google Docs (see example [here](https://docs.google.com/document/d/1ltmc3sialqIavRcIFKR2YTbnJlpmEsBvBT7t-yiqMbM/edit) -- gathered by Merilin[^note1])

[^note1]: **Question for Merilin** Do we have an assessment of the results of each group for each of the activities? if so, can you provide those (e.g., in some sort of Excel sheet) so that we can see which groups had better results?


```{r preprocess, cache=TRUE}

####################### MULTIMODAL TRACKER DATA
# The data file(s)
mm_datafiles <- list.files("../data/raw/20171011/", pattern = "*multimodal-tracker*")
#filename <- "./multimodal-tracker-1507639281494.txt"

#TODO: Implement automated download from the raw data repository
# Assume we have all the raw data in ../data/raw
#overall <- data.frame()
acceleration <- data.frame()
beacons <- data.frame()
geolocation <- data.frame()

for(file in mm_datafiles){

   filedata <- preprocessMultimodalTracker(paste("../data/raw/20171011/",file,sep=""))
   
   group <- as.numeric(substr(file,2,2))
   # filedata$overall$group <- group
   # if(nrow(overall)==0) overall <- filedata$overall
   # else overall <- rbind(overall, filedata$overall)
   
   if(!is.null(filedata$acceleration)){
     filedata$acceleration$group <- group
     if(nrow(acceleration)==0) acceleration <- filedata$acceleration
     else acceleration <- rbind(acceleration, filedata$acceleration)
   }

   if(!is.null(filedata$beacons)){
     filedata$beacons$group <- group
     if(nrow(beacons)==0) beacons <- filedata$beacons
     else beacons <- rbind(beacons, filedata$beacons)
   }
   
   if(!is.null(filedata$geolocation)){
     filedata$geolocation$group <- group
     if(nrow(geolocation)==0) geolocation <- filedata$geolocation
     else geolocation <- rbind(geolocation, filedata$geolocation)
   }
}

# print("Geolocation data")
# print(xtabs(~group,data=geolocation))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- geolocation[geolocation$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }
# 
# print("Beacon data")
# print(xtabs(~group,data=beacons))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- beacons[beacons$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }
# 
# print("Acc data")
# print(xtabs(~group,data=acceleration))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- acceleration[acceleration$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }


########################## OBSERVER DATA
# The data file(s)
obs_datafile <- "../data/raw/20171011/Observer sheet - Linnaruum (Responses).xlsx"

obs_data <- read.xlsx(obs_datafile, sheetIndex = 1)
names(obs_data) <- c("timestamp", "group", "StudentA", "StudentB", "StudentC", "StudentD", 
                     "StudentE", "StudentF", "StudentG", "comment")

obs_data <- obs_data[,c(-8,-9)]

# Student view of the observations
student_obs <- melt(obs_data, id=1:2, measure=3:7)
student_obs$disengaged <- as.numeric(grepl(pattern = "disengaged", x = student_obs$value, fixed = TRUE))
student_obs$looking <- as.numeric(grepl(pattern = "Looking", x = student_obs$value, fixed = TRUE))
student_obs$talking <- as.numeric(grepl(pattern = "Talking", x = student_obs$value, fixed = TRUE))
student_obs$resources <- as.numeric(grepl(pattern = "technology", x = student_obs$value, fixed = TRUE))
student_obs$external <- as.numeric(grepl(pattern = "external", x = student_obs$value, fixed = TRUE))
names(student_obs)[[3]]<-"student"
student_obs <- student_obs[,c(1:3,5:9)]


# summary(student_obs)


# Group view of the observations
group_obs <- obs_data[,c("timestamp","group","comment")]
ag_dis <- aggregate(disengaged ~ timestamp+group, data=student_obs, FUN=sum)
ag_look <- aggregate(looking ~ timestamp+group, data=student_obs, FUN=sum)
ag_talk <- aggregate(talking ~ timestamp+group, data=student_obs, FUN=sum)
ag_res <- aggregate(resources ~ timestamp+group, data=student_obs, FUN=sum)
ag_ext <- aggregate(external ~ timestamp+group, data=student_obs, FUN=sum)

group_obs <- merge(group_obs, ag_dis)
group_obs <- merge(group_obs, ag_look)
group_obs <- merge(group_obs, ag_talk)
group_obs <- merge(group_obs, ag_res)
group_obs <- merge(group_obs, ag_ext)
group_obs <- group_obs[order(group_obs$group,group_obs$timestamp),]

group_obs$group <- as.factor(group_obs$group)
group_obs$comment <- as.character(group_obs$comment)
# summary(group_obs)
# names(group_obs)

# ag <- aggregate(cbind(disengaged,looking,talking,resources,external)~group, data=group_obs, FUN = mean)
# ag


############### STUDENT FEEDBACK
# Read and clean the individual activity feedback data
fb1_datafile <- "../data/raw/20171011/Avastame Kadriorgu - Individual activity feedback (Responses).xlsx"

fb1_data <- read.xlsx(fb1_datafile, sheetIndex = 1)
names(fb1_data) <- c("timestamp", "group", "student", "activity", "contribution", "difficulty", "preparedness", "satisfaction", "relevant")
levels(fb1_data$activity) <- c("I", "II", "III", "IV")
fb1_data$contribution <- as.numeric(gsub( " .*$", "", fb1_data$contribution ))
fb1_data$student.id <- paste(fb1_data$group,fb1_data$student)

# Read and clean the final feedback data
fb2_datafile <- "../data/raw/20171011/Avastame Kadriorgu - Final feedback (Responses).xlsx"

fb2_data <- read.xlsx(fb2_datafile, sheetIndex = 1)
names(fb2_data) <- c("timestamp", "group", "student", "strategies", "roles", "overall.satisfaction", "suggestions")
fb2_data$student.id <- paste(fb2_data$group,fb2_data$student)
```

## Data analysis

These six different data sources can be analyzed quantitatively using basic *descriptive statistics*, to see, for instance, what activities the students found most difficult, most/least relevant, what kinds of engagements were most often observed, etc. Using a bit more advanced *unsupervised learning* and *dimensionality reduction* techniques, we can look a bit more deeply whether there were certain engagement or collaboration profiles in different groups (from the observations taken).

The qualitative data from the open questions in the observations and the student questionnaires can also be analyzed using *content analysis* techniques, to see if there are common topics or trends mentioned. These could be complemented with *interviews* with the main teacher of the project day and the other organizing teachers that designed/coordinated each of the activities, to gather the teacher's perspective on the success of the activities, emergent problems, etc.

Furthermore, combinations of multiple of these sources can be used, for instance, to see if there are *correlations* between the outcomes of a group and the engagement or collaboration profile, or between the engagement and the satisfaction with the activities. However, these correlations will probably not be statistically significant, given the small number of groups/students involved (rather, showing trends that can be investigated more deeply later on).

# The events of 11.10.2017

This first pilot event was a relative success. Apparently, most groups were able to follow most of the activities successfully, and many of them appeared to be quite engaged in solving the tasks. As expected in a complex scenario and research effort organized so hastily, there were also deviations with regard to the plans (e.g., the final reflection activity was done the day after, some students wrote their responses in the paper sheets instead of the electronic version, etc.). Nevertheless, quite an amount of data was generated during the Kadriorg project day, which we are currently analyzing. 

However, **your help is needed to make sense of these results and to refine the research process for next time!**

## What data is *actually* available?

* Our six dutiful observers gathered a total of `r nrow(group_obs)` observations for the groups of students (an average of `r ceiling(nrow(group_obs)/6)` per group). The raw observation responses can be seen [here](https://docs.google.com/forms/d/1RzNTJOBE-upwz1EVX7ud9zca8gAO0oIn4QoCwqn0ko8/edit#responses)
* Students provided a total of `r nrow(fb1_data)` responses to the individual activity feedbacks (around 75% of the expected amount). The raw responses can be seen [here](https://docs.google.com/forms/d/1rh7mLNkft-8DTw9TIwDYUIoxaGEi4-wVeAB6YA9Y2Gk/edit#responses)
* Students provided a total of `r nrow(fb2_data)` responses to the final reflection questionnaire (around `r (nrow(fb2_data)/30)*100`% of the expected amount). The raw responses can be seen [here](https://docs.google.com/forms/d/1zkK905f0wq9h8BPrUC4HWL_8iwfC-dpmWbHyhxmaV8M/edit#responses)
* Due to a technical glitch, NO useful geolocation data was recorded outside the classrooms, and the data about the amount of movement of the groups lasted for only a part of the whole event, between 45 minutes and the whole 4 hours
* As noted above, there is the question of whether we have an **assessment of the different group's task results** (i.e., learning outcomes)

## What kinds of questions can we *actually* answer?

* What activities students found most difficult or satisfying or relevant for the purpose?
* How much time did activities take for the different groups (vs. the planned duration)?
* How engaged each student and each group was, and how this engagement evolved over time?
* What is the relation between activity difficulty vs. satisfaction?
* What is the relation between learning outcomes vs. group engagement?
* *Student reflections about the strategies used and role-taking are very short and superficial*
* *There are a few student suggestions about other potential activities that could be done in this project day*

# How can we improve?

Below are just a few open questions we face at this point, about how to improve the whole event. Feel free to add your own, provide comments or opinions, etc.

## Improving the learning scenario

* Use of a technological platform/environment to more effectively guide students, centralize evidence (student responses) and gather more data about the process (including real-time information!) --> use of SmartZoos, Graasp or other environment?
* It looks like some of the activities were not done (students gave up on the poetry activity?). Should we adjust the tasks or their difficulties or expected durations?

## Improving the research process

* The use of a technological platform for all the exercises would provide us with a new data source (the actions of the students recorded by the platform). However, the way we suggest students to use it (e.g., use it as a group vs. individually) will lead to different kinds
* Right now the human effort in terms of observers is quite high, although the information they provide seems the most valuable in terms of understanding the level of engagement and group functioning. Can we find alternative ways of getting similar kinds of information (e.g., using sensors)?
* What other kinds of analysis do you think we should run on the available data?
* What other questions would you be interested in exploring? especially, about the collaborative problem solving, and self-regulated learning aspects.

# References
