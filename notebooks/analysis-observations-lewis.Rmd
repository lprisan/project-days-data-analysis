---
title: "Processing Observation Data from Learning Experiments"
author: "Lewis Marsh and Luis P. Prieto"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(gsheet)
library(knitr)

source("../src/processObservationData.R")
source("../src/mca.R")
source("../src/hmm.R")

raw_data <- as.data.frame(gsheet2tbl("https://docs.google.com/spreadsheets/d/11BPHcSlqwozx3ffOQhQ5i5O8TRVa8xKoaw2uiny8k5A/edit"))

nstates <- c(1,2,3,4,5,6)
logLik <- c(-13668.08,-12857.04,-12489.29,-12225.95,-12068.50, -11813.53)
AIC <- c(27348.16,25744.08,25030.57,24529.90,24245.00,23769.06)
BIC <- c(27385.87,25838.34,25193.96,24774.99,24584.34,24215.24)
```

This R Markdown report aims to summarise the analyses done on the observation data of several sessions of innovative learning activities, from which multimodal data has been gathered

Note: in order to compile this report and to run the `auto_encoder.R` file, one must install the `keras` package
properly. I.e., one must call `install_keras()` prior to compiling/running.

# Observation data

## Preprocessing

The preprocessing is taken care by the R script in `src/processObservationData.R`. It consists of two main functions, `processObservationData` (to process the data of one session) and `processAllObservationData` (for multiple sessions).

The first of these two functions,
```
data <- processObservationData(raw_data,
                       date=as.POSIXct(strptime("10-01-2018", "%d-%m-%Y")),
                       project="Isle", 
                       activitycol=F,
                       observercol=F,
                       namecols=c("timestamp","group",
                            "StudentA","StudentB","StudentC","StudentD"))
```
takes in a dataframe with the raw observation data, together with some other parameters about the structure of the raw dataset and expected columns in the processed dataset, and returns a clean data frame with the observations for each individual.

In the orginal data sheets, each entry lists the observation for a whole group of students (between 3 and 6
usually) and the observations are listed as a piece of (predefined) text, such as "Talking with their group peers
to solve the task" or "Totally disengaged".

In the output this is broken up into individual entries for each student, for which a student variable is
introduced. Furthermore, each possible observation is turned into a boolean variable for each of these entries,
which makes it easier to conduct further statistical analysis of the dataset. Also, a column
with additional notes from the original dataframe is dropped in the output, as it is difficult to make use of
in a statistical test, and a global ID is created for each student, using the date, group, student number in the
group and the project name. This will be of use when merging several datasets with the second function.

As students undertook several tastks in some of the experiments/projects, the input variable "activitycol" indicates whether the function has to look for an extra activity column in the input data. This will be taken over
one-to-one in the output or will be filled in with "Standard" if there was only one activity.
Similarly, in some projects there were more than one observer placed onto each group to enable to test the
reliability of the observers and the data they produce. The variable "observercol" tells the function whether
there is a separate observer column to take over from the input data. Otherwise "1-A" is filled in for each
observer.

The second function, `processAllObservationData` takes in a vector of URLs in which the GoogleDocs
containing the experimental data are stored in. It reads the data from the online sheets and places
them in a dataframe. By examining the names of the columns in the dataframe, it detects whether there
are activity or observation columns. By inspecting the first time stamp, it determines the date of the experiment.
It then cleans the data using the first function and adds it to the end of a larger dataframe which
eventually gets returned after the function has iterated through all URLs.

### Example of Output
To exhibit what the functions do, there is a brief example. We are given a data frame taken from
one of the GoogleDocs:
```{r first, include=FALSE}
raw_data <- as.data.frame(gsheet2tbl("https://docs.google.com/spreadsheets/d/11BPHcSlqwozx3ffOQhQ5i5O8TRVa8xKoaw2uiny8k5A/edit"))
```
```{r second}
raw_data[1,1:2]
raw_data[1,3]
```

These are only two snippets of the data, as anything else would not fit onto this page.
We see that the first two columns give a timestamp and the group. We furthermore see the oberservation for
student A. There are 3 other students in the group with individual columns as well as
a column for further comments.

By applying the `processObservationData` function, we then see how the student variable is created
and the observations are turned from text into boolean values. Furthermore, the activity and
observer column have been created which makes it easier to merge and compare this data with other datasets.
```{r third}
processed_data <- processAllObservationData()
summary(processed_data)
str(processed_data)
```

### Spot checking the data

Some tables to see what kind of data we have

```{r fourth}

table(processed_data$date, processed_data$student)

```


## MCA

The `mca.R` contains several functions to create and interpret an MCA conducted on the
idicator variables created above.

Firstly, `mca_analysis(data, plots = T)` returns an mca element of the `FactoMineR` package.
It has the options to create several plots, such as a bar plot of the significance of each newly created
MCA dimension, the variance of the old variables in the first two dimensions and the distribution of
each variable from the data set across the first two dimensions. An example of the last instance:

```{r fifth, include = T}
  mca <- mca_analysis(processed_data, plots = T)
#  fviz_ellipses(mca, "disengaged", geom = "point")
```

From these plots, we can see that **Dim1 could be interpreted as some general value of "(not) active engagement"**, while the other dimensions are harder to interpret from looking at the responses alone.

---

A further function is `add_mca_dimensions(dataframe, mca = NULL, dimensions = 3)`. It merges
a chosen amount of dimension generated by the MCA into the original dataframe. One can choose
whether one wants to generated the MCA object separately using the first functions
(e.g. to inspect the plots first) or let the function conduct the MCA itself (if no mca is passed).
For example:

```{r sixth, include = T}
  processed_data <- add_mca_dimensions(processed_data, mca = mca, dimensions = 3)
```

There are several functions that visualise the distribution of our given data points. All of these take
in a dataframe to which the MCA dimensions have been added.

For instance `create_dimension_histograms(dataframe, dimensions = 3)` creates a series of histograms
that exhibit the distribution of data points on a desired amount of variables (by default the first 3).
Similarly thereto, `reate_date_histograms(dataframe, dimension = 1)` creates histograms that show
the distrbution of data points on a chosen dimension with each plot representing one day.

The information conatined in the histograms distingushing dates can also be shown in one single plot,
using the `create_violin_plot` function. For our specific data set, this will looke like:

```{r seventh}
  create_violin_plot(processed_data, dimension = 1)
```

Finally, the function `aggregate_by_groups(dataframe)` allows us to aggregate the dataframe
on groups and timestamps, summing up the MCA dimension.

```{r eighth}
  group_data <- aggregate_by_groups(processed_data)
  summary(group_data)
  
  group_data %>%
    ggplot(aes(x = MCAdim1)) + 
    theme_minimal() + 
    geom_density()
```


## Visual inspection of observer comments

Let's take a look at the observer comments for the highest-lowest values of this "(not) active engagement":

```{r comments, eval=F}
knittr::clean_cache()

group_data %>% 
  filter(!is.na(.data$comments)) %>% 
  filter(MCAdim1 > 1.2) %>% 
  select(.data$MCAdim1,.data$comments) %>% 
  kable


group_data %>% 
  filter(!is.na(.data$comments)) %>% 
  filter(MCAdim1 < -0.6) %>% 
  select(.data$MCAdim1,.data$comments) %>% 
  kable


```

We see that largely, in the first comments the groups are largely disengaged or doing off-task stuff. And conversely, the second ones are mostly about groups largely working on tasks (even if they have problems in doing so)

## Topic modeling of observer comments

We can also do some unsupervised learning on the open comments left by the observers, to try and see if there are certain kinds of words/topics that are distinctly important in them:

```{r lda, warning=F, message=F}

source("../src/lda.R")

plotNrTopics(docids = paste(group_data$timestamp,group_data$student),texts = group_data$comments, addstop = c(NA,"group"))

lda <- getLDA(docids = paste(group_data$timestamp,group_data$student),texts = group_data$comments, addstop = c(NA,"group"), k = 2)

labels <- getTopicLabels(lda, n = 7) # Topic names, based on the most important words

plotLDATopics(lda, n = 7) 

gammas <- tidy(lda, matrix = "gamma") # Probability of each document being in each topic

#detach("package:tidyverse", unload=TRUE)
```

## Hidden Markov Models in the Data Set

Another way of trying to detect structure in our data set is the theory of Hidden Markov Models. In
the file `hmm.R` one can find several functions to generate Hidden Markov Models on our data set.
The most central one is
```{}
create_hmm_states_global(data, max_states = 3, sorted = F, visualise = T).
```
It takes in a preprocessed data set (possibly but not neccesarily with added MCA dimensions),
a maximal number of states the function should try, an indicator if the data set is already sorted
(by global.id and timestamp), and an option for a visualisation of how the function determined the best
number of states. The function then returns a fitted HMM model of the `depmix` class.
We exhibit an example of such a visulisation here (although we do not use the function to reduce computational
expense):


```{r}
# This currently takes about 10 min to run. 7 states become too computationally expensive
create_hmm_states_global(processed_data, visualise = TRUE, max_states = 6)
```

Alternatively, it is also possible to train a Hidden Markov Model on the timeseries of merely one student,
if this is desired. The function to use is `create_hmm_states_student(data, globalid, max_iterations = 3)`.

Furthermore, it is possible to add the transition probabilities and predicted states to the data set, using
the function `insert_hmm(data, initial_state = 1)`. It requires an initial state of the model to be
passed on in order to make the predictions. Especially if little information on what the states represent
is available, this might have to be guessed.

Finally, the function `plot_development_by_student(data, globalid)` allows to compare the first
MCA dimension with the transition through the different HMM states. As arguments a preprocessed
data set augmented by MCA dimensions and HMM states, as well as the global.id of the chosen student
are required. This is exhibited by an example here:

```{r, include = T}
processed_data <- add_mca_dimensions(processed_data)
processed_data <- insert_hmm(processed_data)
plot_development_by_student(processed_data, "Isle 2017-12-06 Group 2 Student D")
```

## Inspection of Created States

As the number of different states is quite modest, it turns out to be difficult to detect any kind of pattern
in them through visual inspection. However, we may still draw some results from checing how the different
observation variables are distributed over the states Using a model with three categories, we for
example find:

```{r, include = F}
cat1 <- subset(processed_data, HMMPredS ==1)[,c("comments", "disengaged", "looking", "talking", "technology", "resources", "external")]
cat2 <- subset(processed_data, HMMPredS ==2)[,c("comments", "disengaged", "looking", "talking", "technology", "resources", "external")]
cat3 <- subset(processed_data, HMMPredS ==3)[,c("comments", "disengaged", "looking", "talking", "technology", "resources", "external")]
```
```{r, include = T}
summary(cat1)
summary(cat2)
summary(cat3)
```

State 1 has high values for talking, technology and external while it is low on looking and disengaged.
Similarly, state 2 scores low in looking and disengaged, as well as in external and technology.
At the same time it takes the highest value in talking out of the three categories.
Finally, state 3 the highest scores in disengaged and looking, while is scores low in all other
observation variables.

We can interpret this as state one roughly representing students who at a given time engage in their task,
but do so by focusing on the use of technology and external resources, while state 2 represents those
who engage by communicating with their group. It has to be noted that state 1 is the largest by far,
so we should assume that there is (at least) a moderate level of communication done by these students
(as otherwise would imply that the majority of students remaind silent in the experiment).
This is also backed by a relatively high mean in the talking variable.
On the other hand state 3 seems to represent students who are disengaged and are "staring
holes into the air".

Furthermore, we like to look at how the results of our HMM are correlated to the results of our MCA.
```{r, include = T}
cor(processed_data[,c("MCAdim1","MCAdim2","MCAdim3","HMMPredS","HMMS1","HMMS2","HMMS3")],
      use="complete.obs", method="kendall")
```

There is moderately high correlation between our predicted state and the first MCA dimension.
However, the highest correlation between our two analyses is between the first MCA dimension
and the transition probability to state 3 (which is close to 0.7).
In other words, if the first MCA dimension score is low, a student is unlikely to fall into
state 3 and vise versa. Going by our interpretations above and in previous sections, this is essentially saying
that if a student is engeged, no matter how efficiently, they are likely to stay engaged.
On the other hand, the low correlation between MCA dimension one and the transition probablity to state
two implies that the nature of engagement of a student may vary.

Furthermore, we can see that the distribution of states is quite different accross the different days:

```{r}
par(mfrow=c(1,7))

for(day in unique(processed_data$date)){
  day_data <- subset(processed_data, date == day)[, c("date", "HMMPredS")]
  counts <- c(nrow(subset(day_data, HMMPredS == 1)),nrow(subset(day_data, HMMPredS == 2)),nrow(subset(day_data, HMMPredS == 3)))
  barplot(counts, main = day)
}
```


## Autoencoding
The file `àuto_encoder.R` contains two functions to build an autoencoder. The first, `build_autoencoder(data)`,
takes in a dataframe and returns a trained autoencoder. The dataset required the usual observation variables
disengaged, looking, talking, technology, resources and external. It returns a model with one hidden layer
containing 5 variables.

The second function `get_predictions(data, model)` takes in a dataframe as above and returns the dataframe
with predictions added for each observation variable, as well as a count of how many variables have been
predicted incorrectly. The latter column is called `AEerr`.

We can exhibit the behaviour of this function:
```{r, include=FALSE}
source("../src/auto_encoder.R")
```

```{r}
training_and_test <- processed_data %>% filter(row_number() <= nrow(processed_data)*0.75)
validation <- processed_data %>% filter(row_number() > nrow(processed_data)*0.75)

model <- build_autoencoder(training_and_test)

predictions <- get_predictions(validation, model)

summary(predictions$AEerr)
```

Once we are satisfied with our model, we can add the units of the hidden layer to our dataframe, using the
`insert_ae_units(data, model)` from the file.
```{r}
processed_data <- insert_ae_units(processed_data, model)
```

We can see that the new variables have a moderate correlation with the previously introduced MCA dimensions.
This does not really come as a surprise, as both techniques are related to Principal Component Analysis (PCA).
```{r}
cor(processed_data[,c("MCAdim1","MCAdim2","MCAdim3","AEdim1","AEdim2","AEdim3")], use="complete.obs", method="kendall")
```



