---
title: "HIK Project Day -- Linnaruum: Day 1 (11.10.2017)"
subtitle: "Research Report"
author: "Luis P. Prieto, María Jesús Rodríguez-Triana, Shashi Kant Shankar and Terje Väljataga"
abstract: "This document describes the research questions and data gathered for the first session of the HIK project days with activities outside the classroom (dubbed 'Linnaruum'), that took place on the 11 October 2017. It also includes the analysis of evidence gathered that day, to illuminate those research questions. Finally, the report includes recommendations and proposals for the research and learning design of future iterations of this learning experience."
bibliography: project-days.bib
always_allow_html: yes
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, error = F, digits=3)

inline_hook <- function(x) {
  if (is.numeric(x)) {
    format(x, digits = 3)
  } else x
}
knitr::knit_hooks$set(inline = inline_hook)

library(ggplot2)
library(data.table)
library(xlsx)
library(reshape2)
library(FactoMineR)
library(factoextra)
library(GGally)
library(timevis)

source("../src/preprocessMultimodalTracker.R")

```

As part of its activities to promote educational innovation among local schools, TLU's Center for Innovation in Education (HIK, in Estonian) organizes a series of 'project days', learning experiences for students at different levels of primary/secondary education, that are inter-disciplinary and project-oriented. These activities are usually designed and enacted by HIK's didactics teachers, and offered to classes of local schools multiple times throughout the academic course.

This report is part of a joint collaboration between HIK and the TLU's CEITER project, to turn these project days into the subject of a co-created design-based research and innovation process. More concretely, it reports about the first pilot of these events that took place on October 11th, 2017. This project day had as a topic the exploration of a concrete urban space (Tallinn's Kadriorg neighborhood), and included both classroom and outdoor activities in the neighborhood itself.

The goal of this report is to **report on the first data analyses** done on the evidence from the first pilot. Both HIK and CEITER will engage in making sense of these results and use them as a basis to decide **how to best organize the next iterations of this project day**, both from the learner experience and from the research perspectives.

In the following sections, we summarize briefly: a) the initial set of research questions posed; b) the methodology used to collect and analyze data; c) what happened on the day of the event, what data has been gathered, and what are the results of the proposed analysis to answer the aforementioned questions; and finally, d) proposes several recommendations about the design of the activities and the research setup for the next iterations.

# Research questions

Although in the preparatory meetings between HIK and CEITER researchers there were no clear, concrete research question expressed, several topics were mentioned as interesting and worthy of exploration, especially around two areas:

* **RQ1**: Naturally, there were mentions of finding out whether the activities were successful in teaching something to the students, and were considered relevant for that purpose (i.e., more **content-related learning outcomes** and the improvement of the **learning design** of the activity)
* **RQ2**: Teachers showed an interest in exploring the **collaborative problem solving (CPS)** of students, which often includes issues such as participation, role distribution or role-taking, etc.
* **RQ3**: Teachers also highlighted an interest on **self-regulated learning** aspects, including the ability to follow instructions autonomously, reflecting on their own learning strategies, engagement and flow, etc.

# Methodology

The overall collaboration (expected to span the activities of the whole year) can be described as a design-based research [@wang2005design] co-created between researchers and practitioners working on an authentic educational setting (the 'HIK project days' events).

In this first iteration, given the exploratory nature of the study at this stage, and the complexity of the phenomena of interest (mentioned in the research questions above), we chose a *mixed method approach* [@creswell2013research], combining both qualitative and quantitative data gathering and analysis. This combination of data sources, and the inclusion of sensors to understand the physical space around the activities, make this research part of the *multimodal learning analytics* field [@blikstein2013multimodal].

## Data gathering

The following techniques were used to gather data about the learners and the learning activities, at different levels:


**Table 1. Overview of data sources**

| **Social level** | **Frequency** | **Data source** | **Focus** |
|:---------|:---------|:---------|:------------------------------------|
| Individual &amp; Group | Every 5 min | [Observations](https://docs.google.com/document/d/1-TinmdR5T6Hyk-Hwac5M8t6r0KdNdh8JJu40rz2i_ao/edit) | Keep track of the individual engagement based on observable interactions. Also comments at the group level were gathered. |
| Individual | Per activity | [Questionnaire](https://docs.google.com/forms/d/e/1FAIpQLSf4CoYnq1acQb7f1nF7PCajSvLnMfKCQhVSYQqx2eHqOkH3Og/viewform) | Self reflection: about the activity, its difficulty and relevance, as well as their perceived individual contribution in the collaboration process |
| Individual | At the end of the whole activity | [Questionnaire](https://docs.google.com/forms/u/1/d/e/1FAIpQLScHyFD1p2WBqVk5FDiyVkk-LdpVucYs1oDtpx9-vJqSbGIdOA/viewform) | Gather the overall impression of the activities, and a short reflection about their problem solving strategies and role-taking |
| Group | Constantly | Beacons + tablets with tracking software | Record the (approximate) movement of the groups including  the amount of movement, GPS data, and indoor location |
| Group | Per activity | [Outcomes](https://docs.google.com/document/u/1/d/1ltmc3sialqIavRcIFKR2YTbnJlpmEsBvBT7t-yiqMbM/edit) | Assessment |


```{r preprocess, cache=TRUE}

####################### MULTIMODAL TRACKER DATA
# The data file(s)
mm_datafiles <- list.files("../data/raw/20171011/", pattern = "*multimodal-tracker*")
#filename <- "./multimodal-tracker-1507639281494.txt"

#TODO: Implement automated download from the raw data repository
# Assume we have all the raw data in ../data/raw
#overall <- data.frame()
acceleration <- data.frame()
beacons <- data.frame()
geolocation <- data.frame()

for(file in mm_datafiles){

   filedata <- preprocessMultimodalTracker(paste("../data/raw/20171011/",file,sep=""))
   
   group <- as.numeric(substr(file,2,2))
   # filedata$overall$group <- group
   # if(nrow(overall)==0) overall <- filedata$overall
   # else overall <- rbind(overall, filedata$overall)
   
   if(!is.null(filedata$acceleration)){
     filedata$acceleration$group <- group
     if(nrow(acceleration)==0) acceleration <- filedata$acceleration
     else acceleration <- rbind(acceleration, filedata$acceleration)
   }

   if(!is.null(filedata$beacons)){
     filedata$beacons$group <- group
     if(nrow(beacons)==0) beacons <- filedata$beacons
     else beacons <- rbind(beacons, filedata$beacons)
   }
   
   if(!is.null(filedata$geolocation)){
     filedata$geolocation$group <- group
     if(nrow(geolocation)==0) geolocation <- filedata$geolocation
     else geolocation <- rbind(geolocation, filedata$geolocation)
   }
}

# print("Geolocation data")
# print(xtabs(~group,data=geolocation))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- geolocation[geolocation$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }
# 
# print("Beacon data")
# print(xtabs(~group,data=beacons))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- beacons[beacons$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }
# 
# print("Acc data")
# print(xtabs(~group,data=acceleration))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- acceleration[acceleration$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }


########################## OBSERVER DATA
# The data file(s)
obs_datafile <- "../data/raw/20171011/Observer sheet - Linnaruum (Responses).xlsx"

obs_data <- read.xlsx(obs_datafile, sheetIndex = 1)
names(obs_data) <- c("timestamp", "group", "StudentA", "StudentB", "StudentC", "StudentD", 
                     "StudentE", "StudentF", "StudentG", "comment")

obs_data <- obs_data[,c(-8,-9)]

# Student view of the observations
student_obs <- melt(obs_data, id=1:2, measure=3:7)
student_obs$disengaged <- as.numeric(grepl(pattern = "disengaged", x = student_obs$value, fixed = TRUE))
student_obs$looking <- as.numeric(grepl(pattern = "Looking", x = student_obs$value, fixed = TRUE))
student_obs$talking <- as.numeric(grepl(pattern = "Talking", x = student_obs$value, fixed = TRUE))
student_obs$resources <- as.numeric(grepl(pattern = "technology", x = student_obs$value, fixed = TRUE))
student_obs$external <- as.numeric(grepl(pattern = "external", x = student_obs$value, fixed = TRUE))
names(student_obs)[[3]]<-"student"
student_obs <- student_obs[,c(1:3,5:9)]


# summary(student_obs)


# Group view of the observations
group_obs <- obs_data[,c("timestamp","group","comment")]
ag_dis <- aggregate(disengaged ~ timestamp+group, data=student_obs, FUN=sum)
ag_look <- aggregate(looking ~ timestamp+group, data=student_obs, FUN=sum)
ag_talk <- aggregate(talking ~ timestamp+group, data=student_obs, FUN=sum)
ag_res <- aggregate(resources ~ timestamp+group, data=student_obs, FUN=sum)
ag_ext <- aggregate(external ~ timestamp+group, data=student_obs, FUN=sum)

group_obs <- merge(group_obs, ag_dis)
group_obs <- merge(group_obs, ag_look)
group_obs <- merge(group_obs, ag_talk)
group_obs <- merge(group_obs, ag_res)
group_obs <- merge(group_obs, ag_ext)
group_obs <- group_obs[order(group_obs$group,group_obs$timestamp),]

group_obs$group <- as.factor(group_obs$group)
group_obs$comment <- as.character(group_obs$comment)
# summary(group_obs)
# names(group_obs)

# ag <- aggregate(cbind(disengaged,looking,talking,resources,external)~group, data=group_obs, FUN = mean)
# ag


############### STUDENT FEEDBACK
# Read and clean the individual activity feedback data
fb1_datafile <- "../data/raw/20171011/Avastame Kadriorgu - Individual activity feedback (Responses).xlsx"

fb1_data <- read.xlsx(fb1_datafile, sheetIndex = 1)
names(fb1_data) <- c("timestamp", "group", "student", "activity", "contribution", "difficulty", "preparedness", "satisfaction", "relevant")
levels(fb1_data$activity) <- c("I", "II", "III", "IV")
fb1_data$contribution <- as.numeric(gsub( " .*$", "", fb1_data$contribution ))
fb1_data$student.id <- paste(fb1_data$group,fb1_data$student)

# Read and clean the final feedback data
fb2_datafile <- "../data/raw/20171011/Avastame Kadriorgu - Final feedback (Responses).xlsx"

fb2_data <- read.xlsx(fb2_datafile, sheetIndex = 1)
names(fb2_data) <- c("timestamp", "group", "student", "strategies", "roles", "overall.satisfaction", "suggestions")
fb2_data$student.id <- paste(fb2_data$group,fb2_data$student)
```

## Data analysis -- TODO!!

These six different data sources can be analyzed quantitatively using basic *descriptive statistics*, to see, for instance, what activities the students found most difficult, most/least relevant, what kinds of engagements were most often observed, etc. Using a bit more advanced *unsupervised learning* and *dimensionality reduction* techniques, we can look a bit more deeply whether there were certain engagement or collaboration profiles in different groups (from the observations taken).

The qualitative data from the open questions in the observations and the student questionnaires can also be analyzed using *content analysis* techniques, to see if there are common topics or trends mentioned. These could be complemented with *interviews* with the main teacher of the project day and the other organizing teachers that designed/coordinated each of the activities, to gather the teacher's perspective on the success of the activities, emergent problems, etc.

Furthermore, combinations of multiple of these sources can be used, for instance, to see if there are *correlations* between the outcomes of a group and the engagement or collaboration profile, or between the engagement and the satisfaction with the activities. However, these correlations will probably not be statistically significant, given the small number of groups/students involved (rather, showing trends that can be investigated more deeply later on).

**Table 2. Data Analyses -- TODO: complete with all the research questions, topics/concrete questions and analyses used??**

| **Type of analyses** | **Technique** | **Aspect to be analysed** |
|:---------|:---------|:------------------------------------|
| Quantitative | Descriptive statistics | Students' perception of the activities (difficult, most/least relevant, what kinds of engagements were most often observed, etc) |
| Quantitative | Unsupervised learning and dimensionality reduction | Extraction of engagement or collaboration profiles in different groups |
| Quantitative | Statistical analyses | Correlations between the outcomes of a group and the engagement or collaboration profile, or between the engagement and the satisfaction |
| Qualitative | Content analyses | Detection of common topics or trends mentioned |



# Analysis results for 11.10.2017

This first pilot event was a relative success. Apparently, most groups were able to follow most of the activities successfully, and many of them appeared to be quite engaged in solving the tasks. As expected in a complex scenario and research effort organized so hastily, there were also deviations with regard to the plans (e.g., the final reflection activity was done the day after, some students wrote their responses in the paper sheets instead of the electronic version, etc.). Nevertheless, quite an amount of data was generated during this first Kadriorg project day: 

* Six observers gathered a total of `r nrow(group_obs)` observations for the groups of students (an average of `r ceiling(nrow(group_obs)/6)` per group), approximately one every 5 minutes.
* Students provided a total of `r nrow(fb1_data)` responses to individual activity feedback forms (around 75% of the expected amount).
* Students provided a total of `r nrow(fb2_data)` responses to the final reflection questionnaire (around `r (nrow(fb2_data)/30)*100`% of the expected amount).
* Due to a technical glitch, NO useful geolocation data was recorded outside the classrooms, and the data about the amount of movement of the groups lasted for only a part of the whole event, between 45 minutes and the whole 4 hours
* The teacher(s) have provided an assessment of the learning outcomes of the activities done by the different groups **PENDING MERILIN!**
* The teacher(s) provided their own perceptions of the results of the activities, see the final section of [this document](https://docs.google.com/document/d/1-TinmdR5T6Hyk-Hwac5M8t6r0KdNdh8JJu40rz2i_ao/edit#).


## Content-related learning outcomes (incl. learning design effectiveness) (RQ1)

### Learning outcomes

**TODO: Pending Merilin**

### Student perception of activities

The **overall perception of the activities** (from the individual activity feedback given in the moment, and the final reflection responses given the following day) is rather positive, as we can see in the graphs below:

```{r overall-fb}
plotdata <- fb1_data[,2:10]
melted <- melt(plotdata, id.vars = c(1:3,9), measure.vars = 4:8)

melted_cont <- melted[melted$variable=="contribution",]
melted_rest <- melted[melted$variable!="contribution",]

# Overall distribution of contributions, satisfaction, difficulty, preparedness
# ggplot(melted_cont, aes(x=value))+geom_density()+theme_minimal()+xlim(0,100)
ggplot(melted_rest, aes(x=value))+geom_histogram(fill="lightgrey",bins=10)+theme_minimal()+scale_x_continuous(breaks=seq(0,9,1), limits=c(0,9))+ggtitle("Student perception of activities, in-the-moment")+facet_grid(~ variable)


ggplot(fb2_data, aes(x=overall.satisfaction))+geom_histogram(fill="grey",bins=10)+theme_minimal()+scale_x_continuous(breaks=seq(0,9,1), limits=c(0,9))+ggtitle("Overall satisfaction, next day reflection")

```

A couple of effects are worth mentioning from the graphs above: 

* the overall satisfaction was quite high in the moment (median: `r median(fb1_data$satisfaction)`), but the next day the perception was a bit less positive (median `r median(fb2_data$overall.satisfaction)`, and less students even cared to respond).
* the values of how difficult and how relevant the activities were perceived to be are quite spread out (and, let's remember, many students did not complete the activity about the poetry). Probably the activities can be polished a bit to make them more relevant, and quite difficult (but not too much).

Even more interesting to polish the activity design, we can look at **how the students perceived each activity**:

```{r activity-fb, fig.height=4, fig.width=8}
# # difficulty, preparedness and satisfaction per activity
ggplot(melted_rest, aes(x=variable, y=value))+geom_boxplot(aes(fill=activity))+theme_minimal()+scale_y_continuous(breaks=seq(0,9,1), limits=c(0,9))
# Are differences significant?

# summary(lm(difficulty~activity, data=fb1_data)) # Activity II seems to be considerably easier
# anova(lm(difficulty~activity, data=fb1_data))
# 
# summary(lm(preparedness~activity, data=fb1_data)) # no big differences in preparedness
# anova(lm(preparedness~activity, data=fb1_data))
# 
# summary(lm(satisfaction~activity, data=fb1_data)) # no big differences in satisfaction
# anova(lm(satisfaction~activity, data=fb1_data))
# 
# summary(lm(relevant~activity, data=fb1_data)) # no big differences in relevancy
# anova(lm(relevant~activity, data=fb1_data))


# # relation between difficulty, preparedness and satisfaction (flow constructs)
# ggplot(fb1_data, aes(x=difficulty, y=satisfaction, col=activity))+geom_jitter(size=4, alpha=0.3)+geom_smooth(se=F)
# ggpairs(fb1_data[,6:9],
#         lower = list(continuous = wrap("points", alpha = 0.1)))+theme_minimal()
```

As we can see, there are small differences among the values of the different activities, but most of these differences are not statistically significant. The only difference that seems more significant is the fact that activity II (finding a house and doing a selfie) was perceived as considerably easier by students (t=`r summary(lm(difficulty~activity, data=fb1_data))$coefficients['activityII','t value']`, p=`r summary(lm(difficulty~activity, data=fb1_data))$coefficients['activityII','Pr(>|t|)']`). This might explain why students were also most satisfied with that activity. Indeed, we can explore further this relationship between difficulty of the activity and student satisfaction with that activity:

```{r difficulty-satisfaction}
ggplot(fb1_data, aes(x=satisfaction, y=difficulty, col=activity))+geom_jitter(size=3, width=0.3, height=0.3, alpha=0.5)+theme_minimal()+scale_y_continuous(breaks=seq(0,9,1), limits=c(0,9))+scale_x_continuous(breaks=seq(0,9,1), limits=c(0,9))+geom_smooth(method="lm",se=F)


```

As we can see, for all activities the trend is that, the more difficult the activities are, students are less satisfied. This is, on the one hand, logical (nobody likes to do difficult things). On the other hand, the learning design should strive towards challenging tasks (as they can lead to more learning) that are nevertheless satisfying to complete.

Finally, by looking at the responses students provided about **suggestions for future activities** (see [this spreadsheet](https://docs.google.com/spreadsheets/d/1R4EKJ1KYiuMqjT41p72XP4Lbsv_5JrDGfWRhQ_CSRNk/edit#gid=52226403), column G), we see that things like orienteering/scavenger hunts/photohunts were most often mentioned, as well as more technology-oriented suggestions like videos about Kadriorg's past or an app/platform/web with geolocated information about the neighborhood's past (similar to the idea of SmartZoos). There were mentions to "better composed instructions" as well :).

### Timing and orchestration

The most important question in this regard is **how much time did activities take for the different groups** (vs. the planned duration). Using the time when students filled in the activity feedback as limits for each of the activities, we can see the following picture:

```{r timing}
# Timeline of student feedback responses... not as clean as we thought!
timevis(data = data.frame(
  start = fb1_data$timestamp,
  content = fb1_data$activity,
  group = as.numeric(fb1_data$group)),
  groups = data.frame(id = sort(unique(as.numeric(fb1_data$group))), content = sort(levels(fb1_data$group)))
 )

```

We see that calculating how much time each group took is quite messy, as responses for an activity are spread over time a lot (was it because they were responding it late, or they just mis-typed the activity they were responding to?). Let's look at only one of the groups, and map also the observer notes on it: are the two sources really aligned? can we really know what activity they were doing until when?

```{r timing-obs}
# Timeline including also the observer notes... example for group 1 only?
group1_fb <- fb1_data[fb1_data$group=="Group 1",c("timestamp","activity")]
names(group1_fb) <- c("timestamp","text")
group1_fb$source <- "student"
group1_obs <- group_obs[group_obs$group=="Group 1",c("timestamp","comment")]
names(group1_obs) <- c("timestamp","text")
group1_obs$source <- "observer"
group1 <- rbind(group1_fb,group1_obs)
group1$source <- as.factor(group1$source)

timevis(data = data.frame(
  start = group1$timestamp,
  content = group1$text,
  group = as.numeric(group1$source)),
  groups = data.frame(id = sort(unique(as.numeric(group1$source))), content = sort(levels(group1$source)))
 )



```

Also related to this, it is worth noting that observers noticed the main teacher going around the neighborhood in a bike, to check on the different groups of students (and sometimes noticing that she had "lost one group" :)). This might suggest the need for **monitoring functions** in the technological support for the activities (especially about geographical location of the students).

## Collaborative problem solving (RQ2)

### Engagement in the groupwork
* How engaged each student and each group was, and how this engagement evolved over time?

### Relationship with learning outcomes
* What is the relation between learning outcomes vs. group engagement?

## Self-regulated learning (RQ3)

### Attention and instruction-following


### Flow aspects?

difficulty vs. preparedness vs. satisfaction?

### Student self-reflection on strategies

* *Student reflections about the strategies used and role-taking are very short and superficial*

## Other analyses??

# Recommendations

Below are a few recommendations and proposals derived from the results shown above. **Please feel free to add your own conclusions, provide comments or opinions or alternative interpretations of the analysis results above!**

## Improving the learning scenario

* Use of a technological platform/environment to more effectively guide students, centralize evidence (student responses) and gather more data about the process (including real-time information!) --> use of SmartZoos, Graasp or other environment?
* It looks like some of the activities were not done (students gave up on the poetry activity?). Should we adjust the tasks or their difficulties or expected durations?

## Improving the research process

* The use of a technological platform for all the exercises would provide us with a new data source (the actions of the students recorded by the platform). However, the way we suggest students to use it (e.g., use it as a group vs. individually) will lead to different kinds
* Right now the human effort in terms of observers is quite high, although the information they provide seems the most valuable in terms of understanding the level of engagement and group functioning. Can we find alternative ways of getting similar kinds of information (e.g., using sensors)?
* What other kinds of analysis do you think we should run on the available data?
* What other questions would you be interested in exploring? especially, about the collaborative problem solving, and self-regulated learning aspects.

* Include more specialists/researchers in CPS and SRL? we need to make more concrete RQs!


# References
