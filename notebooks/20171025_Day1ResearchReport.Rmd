---
title: "HIK Project Day -- Linnaruum: Day 1 (11.10.2017)"
subtitle: "Research Report"
author: "Luis P. Prieto, María Jesús Rodríguez-Triana, Shashi Kant Shankar and Terje Väljataga"
abstract: "This document describes the research questions and data gathered for the first session of the HIK project days with activities outside the classroom (dubbed 'Linnaruum'), that took place on the 11 October 2017. It also includes the analysis of evidence gathered that day, to illuminate those research questions. Finally, the report includes recommendations and proposals for the research and learning design of future iterations of this project day."
bibliography: project-days.bib
always_allow_html: yes
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, error = F, digits=3)

inline_hook <- function(x) {
  if (is.numeric(x)) {
    format(x, digits = 2)
  } else x
}
knitr::knit_hooks$set(inline = inline_hook)

library(ggplot2)
library(data.table)
library(xlsx)
library(reshape)
library(reshape2)
library(FactoMineR)
library(factoextra)
library(GGally)
library(timevis)
library(ineq)
library(knitr)
#library(timeline)

source("../src/preprocessMultimodalTracker.R")

```

As part of its activities to promote educational innovation among local schools, TLU's Center for Innovation in Education (HIK, in Estonian) organizes a series of 'project days', learning experiences for students at different levels of primary/secondary education, that are inter-disciplinary and project-oriented. These activities are usually designed and enacted by HIK's didactics teachers, and offered to classes of local schools multiple times throughout the academic course.

This report is part of a joint collaboration between HIK and the TLU's CEITER project, to turn these project days into the subject of a co-created design-based research and innovation process. More concretely, it reports about the first pilot of these events that took place on October 11th, 2017. This project day had as a topic the exploration of a concrete urban space (Tallinn's Kadriorg neighborhood), and included both classroom and outdoor activities in the neighborhood itself.

The goal of this report is to **report on the first data analyses** done on the evidence from the first pilot. Both HIK and CEITER will engage in making sense of these results and use them as a basis to decide **how to best organize the next iterations of this project day**, both from the learner experience and from the research perspectives.

In the following sections, we summarize briefly: a) the initial set of research questions posed; b) the methodology used to collect and analyze data; c) what happened on the day of the event, what data has been gathered, and what are the results of the proposed analysis to answer the aforementioned questions; and finally, d) proposes several recommendations about the design of the activities and the research setup for the next iterations.

# Research questions

Although in the preparatory meetings between HIK and CEITER researchers there were no clear, concrete research question expressed, several topics were mentioned as interesting and worthy of exploration:

* **RQ1**: Naturally, there were mentions of finding out whether the activities were successful in teaching something to the students, and were considered relevant for that purpose (i.e., **content-related learning outcomes** and the improvement of the **learning design** of the activity)
* **RQ2**: Teachers showed an interest in exploring the **collaborative problem solving (CPS)** of students, which often includes issues such as participation, role distribution or role-taking, etc.
* **RQ3**: Teachers also highlighted an interest on **self-regulated learning** aspects, including the ability to follow instructions autonomously, reflecting on their own learning strategies, engagement and flow, etc.

# Methodology

The overall collaboration (expected to span the project days' activities of the whole year) can be described as a design-based research [@wang2005design] co-created between researchers and practitioners working on the improvement an authentic educational setting (i.e., these 'HIK project days' events).

In this first iteration, given the exploratory nature of the study at this stage, and the complexity of the phenomena of interest (mentioned in the research questions above), we chose a *mixed method approach* [@creswell2013research], combining both qualitative and quantitative data gathering and analysis. This combination of data sources, and the inclusion of sensors to understand the physical space around the activities, make this research part of the *multimodal learning analytics* field [@blikstein2013multimodal].

## Data gathering

The following techniques were used to gather data about the learners and the learning activities, at different levels:


**Table 1. Overview of data sources**

| **Social level** | **Frequency** | **Data source** | **Focus** |
|:---------|:---------|:---------|:------------------------------------|
| Individual &amp; Group | Every 5 min | [Observations](https://docs.google.com/document/d/1-TinmdR5T6Hyk-Hwac5M8t6r0KdNdh8JJu40rz2i_ao/edit) | Keep track of the individual engagement based on observable interactions. Also free-form comments at the group level were gathered. |
| Individual | At the end of each activity (once) | [Questionnaire](https://docs.google.com/forms/d/e/1FAIpQLSf4CoYnq1acQb7f1nF7PCajSvLnMfKCQhVSYQqx2eHqOkH3Og/viewform) | Student self-report: about the activity, its difficulty and relevance, as well as their perceived individual contribution in the collaboration process |
| Individual | At the end of the whole project day | [Questionnaire](https://docs.google.com/forms/u/1/d/e/1FAIpQLScHyFD1p2WBqVk5FDiyVkk-LdpVucYs1oDtpx9-vJqSbGIdOA/viewform) | Gather the overall impression of the activities, and a short reflection about their problem solving strategies and role-taking |
| Group | Constantly | Beacons + tablets with tracking software (by the observer) | Record the (approximate) movement of the groups including  the amount of movement, GPS data, and indoor location (room) |
| Group | Per activity | [Outcomes](https://drive.google.com/open?id=1Pio8UDC-iDvS6Vv1rR9do1WVS6BwFhwCikeWuuHBR_A) | Teacher's assessment of the responses given by each group of students |


```{r preprocess, cache=TRUE}

####################### MULTIMODAL TRACKER DATA
# The data file(s)
mm_datafiles <- list.files("../data/raw/20171011/", pattern = "*multimodal-tracker*")
#filename <- "./multimodal-tracker-1507639281494.txt"

#TODO: Implement automated download from the raw data repository
# Assume we have all the raw data in ../data/raw
#overall <- data.frame()
acceleration <- data.frame()
beacons <- data.frame()
geolocation <- data.frame()

for(file in mm_datafiles){

   filedata <- preprocessMultimodalTracker(paste("../data/raw/20171011/",file,sep=""))
   
   group <- as.numeric(substr(file,2,2))
   # filedata$overall$group <- group
   # if(nrow(overall)==0) overall <- filedata$overall
   # else overall <- rbind(overall, filedata$overall)
   
   if(!is.null(filedata$acceleration)){
     filedata$acceleration$group <- group
     if(nrow(acceleration)==0) acceleration <- filedata$acceleration
     else acceleration <- rbind(acceleration, filedata$acceleration)
   }

   if(!is.null(filedata$beacons)){
     filedata$beacons$group <- group
     if(nrow(beacons)==0) beacons <- filedata$beacons
     else beacons <- rbind(beacons, filedata$beacons)
   }
   
   if(!is.null(filedata$geolocation)){
     filedata$geolocation$group <- group
     if(nrow(geolocation)==0) geolocation <- filedata$geolocation
     else geolocation <- rbind(geolocation, filedata$geolocation)
   }
}

# print("Geolocation data")
# print(xtabs(~group,data=geolocation))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- geolocation[geolocation$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }
# 
# print("Beacon data")
# print(xtabs(~group,data=beacons))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- beacons[beacons$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }
# 
# print("Acc data")
# print(xtabs(~group,data=acceleration))
# for(i in 1:6){
#   print(paste("Group",i))
#   times <- acceleration[acceleration$group==i,"timestamp"]
#   print(paste("We have samples from ",as.POSIXct(min(times)/1000, origin = "1970-01-01"),"to",as.POSIXct(max(times)/1000, origin = "1970-01-01")))
# }


########################## OBSERVER DATA
# The data file(s)
obs_datafile <- "../data/raw/20171011/Observer sheet - Linnaruum (Responses).xlsx"

obs_data <- read.xlsx(obs_datafile, sheetIndex = 1)
names(obs_data) <- c("timestamp", "group", "StudentA", "StudentB", "StudentC", "StudentD", 
                     "StudentE", "StudentF", "StudentG", "comment")

obs_data <- obs_data[,c(-8,-9)]

# Cleanup invalid data, before 10am like "2017-10-11 09:44:49 GMT"
obs_data <- obs_data[obs_data$timestamp>as.POSIXct("2017-10-11 10:00:00 GMT", origin = "1970-01-01", tz = "GMT"),]


# Student view of the observations
student_obs <- melt(obs_data, id=1:2, measure=3:7)
student_obs$disengaged <- as.numeric(grepl(pattern = "disengaged", x = student_obs$value, fixed = TRUE))
student_obs$looking <- as.numeric(grepl(pattern = "Looking", x = student_obs$value, fixed = TRUE))
student_obs$talking <- as.numeric(grepl(pattern = "Talking", x = student_obs$value, fixed = TRUE))
student_obs$resources <- as.numeric(grepl(pattern = "technology", x = student_obs$value, fixed = TRUE))
student_obs$external <- as.numeric(grepl(pattern = "external", x = student_obs$value, fixed = TRUE))
names(student_obs)[[3]]<-"student"
student_obs <- student_obs[,c(1:3,5:9)]
student_obs$student.id <- paste(student_obs$group,student_obs$student)


# summary(student_obs)


# Group view of the observations
group_obs <- obs_data[,c("timestamp","group","comment")]
ag_dis <- aggregate(disengaged ~ timestamp+group, data=student_obs, FUN=sum)
ag_look <- aggregate(looking ~ timestamp+group, data=student_obs, FUN=sum)
ag_talk <- aggregate(talking ~ timestamp+group, data=student_obs, FUN=sum)
ag_res <- aggregate(resources ~ timestamp+group, data=student_obs, FUN=sum)
ag_ext <- aggregate(external ~ timestamp+group, data=student_obs, FUN=sum)

group_obs <- merge(group_obs, ag_dis)
group_obs <- merge(group_obs, ag_look)
group_obs <- merge(group_obs, ag_talk)
group_obs <- merge(group_obs, ag_res)
group_obs <- merge(group_obs, ag_ext)
group_obs <- group_obs[order(group_obs$group,group_obs$timestamp),]

group_obs$group <- as.factor(group_obs$group)
group_obs$comment <- as.character(group_obs$comment)
# summary(group_obs)
# names(group_obs)

# ag <- aggregate(cbind(disengaged,looking,talking,resources,external)~group, data=group_obs, FUN = mean)
# ag


############### STUDENT FEEDBACK
# Read and clean the individual activity feedback data
fb1_datafile <- "../data/raw/20171011/Avastame Kadriorgu - Individual activity feedback (Responses).xlsx"

fb1_data <- read.xlsx(fb1_datafile, sheetIndex = 1)
names(fb1_data) <- c("timestamp", "group", "student", "activity", "contribution", "difficulty", "preparedness", "satisfaction", "relevant")
fb1_data <- fb1_data[1:89,] # REmove the responses from the second day!
levels(fb1_data$activity) <- c("I", "II", "III", "IV", "V", "VI")
fb1_data$contribution <- as.numeric(gsub( " .*$", "", fb1_data$contribution ))
fb1_data$student.id <- paste(fb1_data$group,fb1_data$student)

# Read and clean the final feedback data
fb2_datafile <- "../data/raw/20171011/Avastame Kadriorgu - Final feedback (Responses).xlsx"

fb2_data <- read.xlsx(fb2_datafile, sheetIndex = 1)
names(fb2_data) <- c("timestamp", "group", "student", "strategies", "roles", "overall.satisfaction", "suggestions")
fb2_data <- fb2_data[1:19,] # Remove the responses from other days!
fb2_data$student.id <- paste(fb2_data$group,fb2_data$student)

######### ASSESSMENTS DATA
# Read and clean the group assessment data
ass_datafile <- "../data/raw/20171011/Outcomes-Assessment.xlsx"
ass_data <- read.xlsx(ass_datafile, sheetIndex = 1)
ass_data <- ass_data[1:6,1:13]
names(ass_data)[1:7] <- c("group","Task.1.Quali.Maps",
                            "Task.2.Quali.Description.Selfie",
                            "Task.3.Quali.Interview",
                            "Task.4.Quali.Poem.Selfie","Task.5.Quali.Wrapup",
                            "Task.6.Quali.Art")
ass_data$group <- as.factor(ass_data$group)
levels(ass_data$group) <- c("Group 1", "Group 2", "Group 3", "Group 4", "Group 5", "Group 6")

########## TIMING DATA
# Read and clean the timing data
tim_datafile <- "../data/raw/20171011/Start_end activity times.xlsx"
tim_data <- read.xlsx(tim_datafile, sheetIndex = 1, colClasses = )
# We fix the dates in the read times
datefixer <- function(x, what){
  library(lubridate)
  olddate <- x[what]
  hours <- hour(olddate) # Adjustment due to timezones?
  minutes <- minute(olddate)
  newdate <- ymd_hms(paste("2017-10-11 ",hours,":",minutes,":00",sep=""))
  newdate
}
tim_data$new.start <- as.POSIXct(apply(tim_data, 1, datefixer, "start"), origin="1970-01-01", tz = "GMT")
tim_data$new.end <- as.POSIXct(apply(tim_data, 1, datefixer, "end"), origin="1970-01-01", tz = "GMT")
tim_data$group <- as.factor(ass_data$group)
levels(tim_data$group) <- c("Group 1", "Group 2", "Group 3", "Group 4", "Group 5", "Group 6")
tim_data <- tim_data[,c(1,2,5,6,7)]
names(tim_data) <- c("group","activity","planned","start","end")
tim_data$duration <- as.numeric(tim_data$end - tim_data$start)
```

## Data analysis

These data sources can be analyzed quantitatively using basic *descriptive statistics*, to see, for instance, what activities the students found most difficult, most/least relevant, what kinds of engagements were most often observed, etc. Using a bit more advanced *unsupervised learning* and *dimensionality reduction* techniques, we can look a bit more deeply into whether there were certain engagement or collaboration profiles in different groups (from the observations taken).

The qualitative data from the open questions in the observations and the student questionnaires can also be analyzed using *content analysis* techniques, to see if there are common topics or trends mentioned. These could be complemented with the teachers' assessment and perspective on the success of the activities, emergent problems, etc.

Furthermore, combinations of multiple of these sources can be used, for instance, to see if there are *correlations* between the outcomes of a group and their engagement or collaboration profile, or between the engagement and the satisfaction with the activities. However, these correlations will probably not be statistically significant, given the small number of groups/students involved (rather, showing trends that can be investigated more deeply later on).

**Table 2. Data Analyses Performed**

| **Research Question** | **Informative question** | **Data source** | **Technique** | **Kind of analysis** |
|:------|:----------------------------|:------|:------|:------|
| RQ1 – Content and Pedagogical design improvement | Which groups performed better?Which activities had better/worse student outcomes? | Teacher&#39;s assessments | Descriptive statistics | Quantitative |
| RQ1 – Content and Pedagogical design improvement | How did students perceive the activities in terms of difficulty/ preparedness/ satisfaction/ relevance? What other activities can be done? | Students&#39; activity questionnaires, Final reflection (day after) | Descriptive statistics, Content analysis | Quantitative + Qualitative |
| RQ1 – Content and Pedagogical design improvement | What is the relationship between students&#39; perceptions of the activities and the learning outcomes? | Teacher&#39;s assessments, Students&#39; activity questionnaires | Correlation tests | Quantitative |
| RQ1 – Content and Pedagogical design improvement | How long did students take to do the activities? | Students&#39; activity questionnaires, Observations, Tracker tablets | Manual matching (!) | Quantitative + Qualitative |
| RQ2 – Collaborative Problem Solving | How engaged was each student/group in the collaborative work? | Observations | Descriptive statistics | Quantitative |
| RQ2 – Collaborative Problem Solving | How engaged were students/groups over time? | Observations | Descriptive statistics | Quantitative |
| RQ2 – Collaborative Problem Solving | How engaged were students/groups in each activity? | Observations, Student activity questionnaires, Tracker tablets | Descriptive statistics | Quantitative |
| RQ2 – Collaborative Problem Solving | Is there are relation between the students&#39; engagements and their learning outcomes? | Observations, Teacher&#39;s assessments | Correlation tests | Quantitative |
| RQ2 – Collaborative Problem Solving | Do different students show different kinds/profiles of participation? | Observations | Cluster analysis | Quantitative |
| RQ2 – Collaborative Problem Solving | How equal was the participation of students within a group? | Observations, Student activity questionnaires | Descriptive statistics, Correlation tests | Quantitative |
| RQ3 – Self-regulated learning | Was there any problem for students to follow the instructions? | Teacher&#39;s assessment, Observations | Content analysis | Qualitative |
| RQ3 – Self-regulated learning | Was there a relationship between the activities&#39; difficulty, preparedness and satisfaction? (flow) | Student activity questionnaires | Correlation tests | Quantitative |
| RQ3 – Self-regulated learning | Did students follow specific strategies to solve the tasks or organize collaboration? | Students&#39; final feedback questionnaire | Content analysis | Qualitative |



# Analysis results for 11.10.2017

This first pilot event was a relative success. Apparently, most groups were able to follow most of the activities successfully, and many of them appeared to be quite engaged in solving the tasks. As expected in a complex scenario and research effort organized so hastily, there were also deviations with regard to the plans (e.g., the final reflection activity was done the day after, some students wrote their responses in the paper sheets instead of the electronic version, etc.). Nevertheless, quite an amount of data was generated during this first Kadriorg project day: 

* Six observers gathered a total of `r nrow(group_obs)` observations for the groups of students (an average of `r ceiling(nrow(group_obs)/6)` per group), approximately one every 5 minutes.
* Students provided a total of `r nrow(fb1_data)` responses to individual activity feedback forms (around 75% of the expected amount).
* Students provided a total of `r nrow(fb2_data)` responses to the final reflection questionnaire (around `r (nrow(fb2_data)/30)*100`% of the expected amount).
* Due to a technical glitch, NO useful geolocation data was recorded outside the classrooms, and the data about the amount of movement of the groups lasted for only a part of the whole event, between 45 minutes and the whole 4 hours
* The teacher(s) have provided an assessment of the learning outcomes of the activities done by the different groups
* The teacher(s) provided their own perceptions of the results of the activities, see the final section of [this document](https://docs.google.com/document/d/1-TinmdR5T6Hyk-Hwac5M8t6r0KdNdh8JJu40rz2i_ao/edit#).


## Content-related learning outcomes (incl. learning design effectiveness) (RQ1)

### Learning outcomes

The first question we can ask is how well did the different groups of students perform in the different activities. From the graph below showing the average score (from 1-poor, 2-satisfactory, 3-good, 4-very well), we see that all groups performed similarly quite well, except for Group 6, that did substantially worse. It will be interesting to follow this group more closely in the next analyses below.

```{r overall-outcomes}
## Summarizes data.
## Gives count, mean, standard deviation, standard error of the mean, and confidence 
## interval (default 95%).
##   data: a data frame.
##   measurevar: the name of a column that contains the variable to be summariezed
##   groupvars: a vector containing names of columns that contain grouping variables
##   na.rm: a boolean that indicates whether to ignore NA's
##   conf.interval: the percent range of the confidence interval (default is 95%)
summarySE <- function(data=NULL, measurevar, groupvars=NULL, na.rm=FALSE, conf.interval=.95) {
    library(doBy)

    # New version of length which can handle NA's: if na.rm==T, don't count them
    length2 <- function (x, na.rm=FALSE) {
        if (na.rm) sum(!is.na(x))
        else       length(x)
    }

    # Collapse the data
    formula <- as.formula(paste(measurevar, paste(groupvars, collapse=" + "), sep=" ~ "))
    datac <- summaryBy(formula, data=data, FUN=c(length2,mean,sd), na.rm=na.rm)

    # Rename columns
    names(datac)[ names(datac) == paste(measurevar, ".mean",    sep="") ] <- measurevar
    names(datac)[ names(datac) == paste(measurevar, ".sd",      sep="") ] <- "sd"
    names(datac)[ names(datac) == paste(measurevar, ".length2", sep="") ] <- "N"
    
    datac$se <- datac$sd / sqrt(datac$N)  # Calculate standard error of the mean
    
    # Confidence interval multiplier for standard error
    # Calculate t-statistic for confidence interval: 
    # e.g., if conf.interval is .95, use .975 (above/below), and use df=N-1
    ciMult <- qt(conf.interval/2 + .5, datac$N-1)
    datac$ci <- datac$se * ciMult
    
    return(datac)
}

# Summarize the assessment with standard error etc
melted_ass <- melt(ass_data[,c(1,8:13)], id.vars = 1)
aggr_ass <- summarySE(melted_ass, measurevar="value", groupvars=c("group"))
ggplot(aggr_ass, aes(x=group, y=value, fill=group)) + 
    geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=value-ci, ymax=value+ci),
                  width=.2,                    # Width of the error bars
                  position=position_dodge(.9))+theme_minimal()+ylim(c(0,4))+
    xlab("Student group")+ylab("Assessment")+ theme(legend.position="none")#+
    #ggtitle("Average teacher assessment per group,\nfrom 1-poor, to 4-very good")

aggr_ass2 <- summarySE(melted_ass, measurevar="value", groupvars=c("variable"))
levels(aggr_ass2$variable) <- c("I maps","II building\n&selfie","III data\ncollecting","IV poem\n&selfie","V discussion","VI artwork")
ggplot(aggr_ass2, aes(x=variable, y=value, fill=variable)) + 
    geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=value-ci, ymax=value+ci),
                  width=.2,                    # Width of the error bars
                  position=position_dodge(.9))+theme_minimal()+ylim(c(0,4))+
    scale_fill_brewer(palette="Set2")+
    xlab("Activity")+ylab("Assessment")+ theme(legend.position="none")
    #ggtitle("Average teacher assessment per activity,\nfrom 1-poor, to 4-very good")

```

The second graph above shows that most groups had problems with Activity 5 (the wrap-up activity), and that everyone performed very good on Activity 6 (the artistic/creative vision). This suggests that maybe the design for these two activities (or the assessment criteria) could be improved.


### Student perception of activities

The students' **overall perception of the activities** (from the individual activity feedback given in the moment, and the final reflection responses given the following day) is rather positive, as we can see in the graphs below:

```{r overall-fb}
plotdata <- fb1_data[,2:10]
melted <- melt(plotdata, id.vars = c(1:3,9), measure.vars = 4:8)

melted_cont <- melted[melted$variable=="contribution",]
melted_rest <- melted[melted$variable!="contribution",]

# Overall distribution of contributions, satisfaction, difficulty, preparedness
# ggplot(melted_cont, aes(x=value))+geom_density()+theme_minimal()+xlim(0,100)
ggplot(melted_rest, aes(x=value))+geom_histogram(fill="lightgrey",bins=10)+theme_minimal()+scale_x_continuous(breaks=seq(0,9,1), limits=c(0,9))+ggtitle("Student perception of activities, in-the-moment")+facet_grid(~ variable)


ggplot(fb2_data, aes(x=overall.satisfaction))+geom_histogram(fill="grey",bins=10)+theme_minimal()+scale_x_continuous(breaks=seq(0,9,1), limits=c(0,9))+ggtitle("Overall satisfaction, next day reflection")

```

A couple of effects are worth mentioning from the graphs above: 

* the overall satisfaction was quite high in the moment (median value: `r median(fb1_data$satisfaction)`), but the next day the perception was a bit less positive (median `r median(fb2_data$overall.satisfaction)`, and less students cared to respond).
* the values of how difficult and how relevant the activities were perceived to be, are quite spread out (and, let's remember, many students did not complete the activity about the poetry). Probably the activities can be polished a bit to make them more relevant, and maybe a bit more difficult (but not too much).

Even more interesting to polish the activity design, we can look at **how the students perceived each activity**. We only have data from questionnaires for the first 4 activities (I-Maps, II-Description&Selfie, III-Interviews, IV-Poem&Selfie):

```{r activity-fb, fig.height=4, fig.width=8}
# # difficulty, preparedness and satisfaction per activity

levels(melted_rest$activity) <-  c("I maps","II building&selfie","III data collecting","IV poem&selfie","V discussion","VI artwork")
ggplot(melted_rest, aes(x=variable, y=value, fill=activity))+geom_boxplot()+theme_minimal()+scale_y_continuous(breaks=seq(0,9,1), limits=c(0,9))+
  ylab("")+xlab("")+guides(fill=guide_legend(title="Activity"))
# Are differences significant?

# summary(lm(difficulty~activity, data=fb1_data)) # Activity II seems to be considerably easier
# anova(lm(difficulty~activity, data=fb1_data))
# 
# summary(lm(preparedness~activity, data=fb1_data)) # no big differences in preparedness
# anova(lm(preparedness~activity, data=fb1_data))
# 
# summary(lm(satisfaction~activity, data=fb1_data)) # no big differences in satisfaction
# anova(lm(satisfaction~activity, data=fb1_data))
# 
# summary(lm(relevant~activity, data=fb1_data)) # no big differences in relevancy
# anova(lm(relevant~activity, data=fb1_data))


# # relation between difficulty, preparedness and satisfaction (flow constructs)
# ggplot(fb1_data, aes(x=difficulty, y=satisfaction, col=activity))+geom_jitter(size=4, alpha=0.3)+geom_smooth(se=F)
# ggpairs(fb1_data[,6:9],
#         lower = list(continuous = wrap("points", alpha = 0.1)))+theme_minimal()
```

As we can see, there are small differences among the values of the different activities, but most of these differences are not statistically significant. The only difference that seems more significant is the fact that activity II (finding a house and doing a selfie) was perceived as considerably easier by students (t=`r summary(lm(difficulty~activity, data=fb1_data))$coefficients['activityII','t value']`, p=`r summary(lm(difficulty~activity, data=fb1_data))$coefficients['activityII','Pr(>|t|)']`). This might explain why students were also most satisfied with that activity. 

<!-- Removed, we tell the same in the RQ3
Indeed, we can see there is a quite strong negative correlation (`r cor(plotdata$satisfaction, plotdata$difficulty)`) between students' perceived difficulty of an activity and their satisfaction with it (i.e., students don't like how they perform in the more difficult activities).-->

```{r perc-corr}

# ggplot(plotdata, aes(x=difficulty, y=satisfaction))+theme_minimal()+geom_jitter(height=0.1, width = 0.1, size=3, alpha=0.3)+geom_smooth(method = "lm")
#cor(plotdata$satisfaction, plotdata$difficulty) # -0.45, Correlation of satisfaction and assessment is negative! Good satisfaction is not good proof of learning...
#cor.test(data_rest$satisfaction, data_rest$difficulty) # Not significant/strong

```


Finally, by looking at the responses students provided about **suggestions for future activities** (see [this spreadsheet](https://docs.google.com/spreadsheets/d/1R4EKJ1KYiuMqjT41p72XP4Lbsv_5JrDGfWRhQ_CSRNk/edit#gid=52226403), column G), we see that things like orienteering/scavenger hunts/photohunts were most often mentioned, as well as more technology-oriented suggestions like videos about Kadriorg's past or an app/platform/web with geolocated information about the neighborhood's past (similar to the idea of SmartZoos). There were mentions to "better composed instructions" as well :).

### Student perception vs. outcomes

We can also see whether there is any relationship between the two aspects above: how students perceived the activities, and how good were each group's learning outcomes:

```{r perc-outcomes}

# Join assessment data with the individual perceptions
levels(melted_ass$variable) <- c("I", "II", "III", "IV", "V", "VI")
names(melted_ass) <- c("group", "activity", "assessment")
melt_melt_ass <- melt(melted_ass, id.vars = c(1,2))

data_rest <- dcast(melted_rest, student.id+group+activity~variable, mean)
# We add the assessment data
lookup <- function(x){melt_melt_ass[melt_melt_ass$group == x['group'] & melt_melt_ass$activity == x['activity'],'value']}
data_rest$assessment <- apply(data_rest, 1, lookup)

ggplot(data_rest, aes(x=satisfaction, y=assessment))+theme_minimal()+geom_jitter(height=0.1, width = 0.1, size=3, alpha=0.3)+geom_smooth(method = "lm")
#cor(data_rest$satisfaction, data_rest$assessment) # -0.18, Correlation of satisfaction and assessment is negative! Good satisfaction is not good proof of learning...
#cor.test(data_rest$satisfaction, data_rest$assessment) # Not significant/strong
ggplot(data_rest, aes(x=difficulty, y=assessment))+theme_minimal()+geom_jitter(height=0.1, width = 0.1, size=3, alpha=0.3)+geom_smooth(method="lm")
#cor(data_rest$difficulty, data_rest$assessment) # Correlation of difficulty and assessment is positive! More difficult activities do not necessarily lead to less performance (rather the opposite)
#cor.test(data_rest$difficulty, data_rest$assessment) # Not significant/strong
```

From the graphs above we see that student satisfaction with their performance is not a good indicator for how they did in the actual assessment according to the teacher -- something other studies have also proven [@bowman2010can; @sitzmann2010self]. Interestingly, students tended to perform better (according to the teacher assessment) in those activities that they perceived as more difficult.


### Activity timing and orchestration

Another important question when deciding how the learning activities should be changed, is **how much time did activities take for the different groups** (vs. the planned duration). However, determining this is not an easy task, as students (even within a same group) filled in the activity questionnaires at oddly different times. Hence, we have (painfully) calculated these durations by hand, triangulating the different data sources (observations, student activity responses, sensors):

```{r timing}
# # Timeline of student feedback responses... not as clean as we thought!
# timevis(data = data.frame(
#   start = fb1_data$timestamp,
#   content = fb1_data$activity,
#   group = as.numeric(fb1_data$group)),
#   groups = data.frame(id = sort(unique(as.numeric(fb1_data$group))), content = sort(levels(fb1_data$group)))
#  )
# 
# # Timeline including also the observer notes... example for group 1 only?
# group1_fb <- fb1_data[fb1_data$group=="Group 6",c("timestamp","activity")]
# names(group1_fb) <- c("timestamp","text")
# group1_fb$source <- "student"
# group1_obs <- group_obs[group_obs$group=="Group 6",c("timestamp","comment")]
# names(group1_obs) <- c("timestamp","text")
# group1_obs$source <- "observer"
# group1 <- rbind(group1_fb,group1_obs)
# group1$source <- as.factor(group1$source)
# 
# timevis(data = data.frame(
#   start = group1$timestamp,
#   content = group1$text,
#   group = as.numeric(group1$source)),
#   groups = data.frame(id = sort(unique(as.numeric(group1$source))), content = sort(levels(group1$source)))
#  )

```

We can try to get some idea about when certain activities happened, from the observer notes and the data about when each group was in each room (tracked by the tablets carried by the observer). For those that happened outside (i.e., no room data), we can try to make an "educated guess" taking into account the student activity questionnaires. This gives us a timeline like the following:

```{r beacons-pos}
# # From https://drive.google.com/open?id=0Bzyi_E7EV3EjRDlYNzFUTGx5c1U
# beacon_lookup <- function(x){
#   minor <- x['minor']
#   room <- NULL;
#   if(minor=="125" | minor=="65407" | minor=="36240") room="T412"
#   else if(minor=="33533") room="T410"
#   else if(minor=="5800") room="T420"
#   else if(minor=="22568") room="T416"
#   else if(minor=="57540") room="T415"
#   else if(minor=="16007" | minor=="25969") room="Corridor"
# 
#   room
# }
# 
# for (g in unique(beacons$group)){
#   beacons_grp <- beacons[beacons$group==g,]
#   unique_beacons <- data.frame()
#   for(t in unique(beacons_grp$timestamp)){
#     beacons_t <- beacons_grp[beacons_grp$timestamp==t,] # Select the registers with a same timestamp, and get the one with strongest signal
#     if(nrow(unique_beacons)==0) unique_beacons <- (beacons_t[which.max(beacons_t$rssi),])[1,]
#     else unique_beacons <- rbind(unique_beacons, (beacons_t[which.max(beacons_t$rssi),])[1,])
#   }
#   unique_beacons$room <- as.factor(apply(unique_beacons,1,beacon_lookup))
#   cutoff_beacons <- unique_beacons[unique_beacons$rssi>-90,] # The threshold is determined by trial and error!
#   print(ggplot(cutoff_beacons, aes(x=as.POSIXct(timestamp/1000, origin = "1970-01-01"),y=room, col=room))+geom_jitter(size=3,alpha=0.3)+theme_minimal()+ggtitle(paste("Group",g)))
# }


# I-Maps, II-Description&Selfie, III-Interviews, IV-Poem&Selfie, V-Wrapup, VI-Art
# The student activity questionnaires are offset -1hr!!
# Activity I: From https://drive.google.com/open?id=0Bzyi_E7EV3EjTGt5REs1UTNreUk
# All start at 10:04 (teacher explanation), 10:10 Start observations
# End times taken from the beacon visualizations, or the observations
# Activity II: 
# Start a the end of the first one. End From the observation or the student questionnaire?

# Activity IV: From https://drive.google.com/open?id=0Bzyi_E7EV3EjTGt5REs1UTNreUk
# Start ...
# All end at 11:55 the latest

# Activity V: From https://drive.google.com/open?id=0Bzyi_E7EV3EjTGt5REs1UTNreUk
# All start at 12:30, end at 13:00 - triangulate with my tablet (Group 5) or Chus (Group 3)?


# Activity VI: From https://drive.google.com/open?id=0Bzyi_E7EV3EjTGt5REs1UTNreUk
# All start at 13:00, end at 13:30 - triangulate with my tablet (Group 5) or Chus (Group 3)?


```

```{r durations, fig.height=4, fig.width=8}

melted_tim <- melt(tim_data, id.vars = c(1,2), measure.vars = 6)
ag_plan <- aggregate(planned~activity, tim_data, mean)
ag_plan$group <- "Planned"
ag_plan$variable <- "duration"
ag_plan$value <- ag_plan$planned
ag_plan <- ag_plan[,c("group","activity","variable","value")]
durations <- rbind(melted_tim,ag_plan)

ggplot(durations, aes(x=variable, y=value, fill=group)) + 
    geom_bar(position=position_dodge(), stat="identity")+
    theme_minimal()+
    ggtitle("Activity durations")+facet_wrap(~activity)
```

As we can see:

* Activity I (Maps) seemed to take roughly the planned amount of time (`r mean(tim_data[tim_data$activity=="I",'duration'])` mins, 35 planned)
* *Activity II (Description & Selfie) took roughly `r mean(tim_data[tim_data$activity=="II",'duration'])` minutes (compared to 25 planned)*
* *Activity III (Interviews) also took less time than expected (`r mean(tim_data[tim_data$activity=="III",'duration'])` mins vs. 45 planned)*
* **Activity IV (Poem & Selfie) took on average `r mean(tim_data[tim_data$activity=="IV",'duration'])` (vs. 15 expected)**
* **Activity V (Wrap-up) actually took more time than expected (about 30 mins, vs. 15 planned)**
* Activity VI (Art) was spot on its alloted 30 mins (probably because time was up and students had to go anyways)

Also related to this, it is worth noting that observers noticed the main teacher going around the neighborhood in a bike, to check on the different groups of students (and sometimes noticing that she had "lost one group" :)). This might suggest the need for **monitoring functions** in the technological support for the activities (especially about geographical location of the students).

## Collaborative problem solving (RQ2)

### Engagement in the groupwork

How engaged was each of the students in the work (and in what way, e.g., talking vs. manipulating resources or getting external help), as noted by the observers? We can see the "engagement profile" of each student, like:

```{r engagement-stu}
# Engagement profile of a student
ag_stu <- aggregate(cbind(disengaged,looking,talking,resources,external)~student.id, data=student_obs, FUN="mean")
meltedstu <- melt(ag_stu)
ggplot(meltedstu, aes(x=as.factor(student.id),y=value))+geom_bar(aes(fill = variable), position = "dodge", stat="identity")+coord_flip()+theme_minimal()

```

This visualization is quite complex, but we can already see that different students have different kinds of engagement: some are more prone to talking (long green bars), some to manipulating resources (long blue bars). We can also do a similar profiling by group: 

```{r engagement-grp}
# Engagement profile of a student
ag_grp <- aggregate(cbind(disengaged,looking,talking,resources,external)~group, data=student_obs, FUN="mean")
meltedgrp <- melt(ag_grp)
ggplot(meltedgrp, aes(x=as.factor(group),y=value))+geom_bar(aes(fill = variable), position = "dodge", stat="identity")+coord_flip()+theme_minimal()+xlab("")+ylab("Proportion of time the behavior was observed\n(avg. of group members)")+guides(fill=guide_legend(title="Engagement behavior"))

```

However, these pictures are still quite complex. One way to try to simplify them is to see whether there are underlying, latent factors in this data. Using [Principal Component Analysis](http://setosa.io/ev/principal-component-analysis/) we can try to extract these latent factors from the numerical properties of the data (e.g., which of the variables tend to vary together), and we obtain the two dimensions shown below:

```{r pca-obs}
pca.student <- PCA(student_obs[,4:8], scale.unit = TRUE, graph = FALSE)
# Interpretation of dimensions: engagement, external focus
student_obs$participation <- pca.student$ind$coord[,1]
student_obs$focusext <- pca.student$ind$coord[,2]
plot(pca.student,choix="var")
```

These two dimensions (the X and Y axes of the figure above) we can interpret as follows:

1. The X axis can be how much each student (or observation, or group) is **participating** (or engaged actively) in the taks (as it correlates with using resources, talking with peers or external people). We can see that in the negative side are the disengaged behaviors, or just looking as others solve the task.
2. The Y axis seems to be more related to **where the attention is directed to**: towards external people (or away from the task) (positive Y axis), or towards the group (either talking or looking).

Using these two indexes (participation and group/external focus), we can draw the profiles of each student -- like saying that Group 5 student D (a foreign student that could not speak Estonian) had low participation (`r mean(student_obs[student_obs$student.id=="Group 5 StudentD","participation"])`), but her focus of attention still was mostly towards the group (`r mean(student_obs[student_obs$student.id=="Group 5 StudentD","focusext"])`). We can also profile the groups, like below:

```{r pca-grp-stu, fig.height=3, fig.width=8}
# Graph... maybe too complex
# fviz_pca_ind(pca.student,
#              geom.ind = "point", # show points only (but not "text")
#              col.ind = student_obs$group, # color by groups
#              addEllipses = TRUE, # Concentration ellipses
#              legend.title = "Groups", ellipse.type = "confidence", repel = T
#              )
#agpca <- aggregate(cbind(participation,focusext)~group, data=student_obs, FUN=mean)
#meltpca <- melt(agpca)
#ggplot(agpca, aes(x=participation, y=focusext, col=group))+geom_point(size=3)

meltpca <- melt(student_obs, id.vars = c("group","student.id"), measure.vars = c("participation","focusext"))

# Optional: filter only groups 2 and 6 for comparison
filteredpca <- meltpca[meltpca$group=="Group 2" | meltpca$group == "Group 6",] 

#ggplot(filteredpca, aes(y=value, x=group, fill=group))+geom_violin()+geom_boxplot(alpha=0.1)+facet_wrap(~variable)+coord_flip()
ggplot(meltpca, aes(y=value, x=group, fill=group))+geom_violin()+geom_boxplot(alpha=0.1)+facet_wrap(~variable)+coord_flip()

```

Above, we can see for example that Group 2 (the one with best average outcomes) has a high profile of participation, quite balanced between external and intra-group focus of attention. We can compare this group with Group 6 (the lowest performers), who had a much lower participation profile.

We can also see, for example, how the engagement varied over time, for each group:

```{r pca-time}
filteredobs <- student_obs[student_obs$group=="Group 2" | student_obs$group=="Group 6",]

#ggplot(student_obs, aes(x=timestamp, y=participation, col=group))+geom_smooth(span = 0.15, se=F)+theme_minimal()#+geom_point(alpha=0.1,size=4)
ggplot(filteredobs, aes(x=timestamp, y=participation, col=group))+geom_smooth(span = 0.15, se=F)+theme_minimal()#+geom_point(alpha=0.1,size=4)
```

... or how the inside/outside focus also varied over time, for group 6, along the times they were doing the different activities:

```{r pca-time2}
#ggplot(student_obs, aes(x=timestamp, y=focusext, col=group))+geom_smooth(span = 0.15, se=F)+theme_minimal()#+geom_point(alpha=0.1,size=4)

filteredobs6 <- filteredobs[filteredobs$group == "Group 6",]
filteredobs2 <- filteredobs[filteredobs$group == "Group 2",]

ggplot(filteredobs6, aes(x=timestamp, y=participation, col=group))+geom_smooth(span = 0.15, se=F)+theme_minimal()+geom_segment(data=tim_data[tim_data$group=="Group 6",], aes(x=start, xend=end, y=-3, yend=-3, col=activity, size=10))
```

### Engagement over time/activities

By joining the activity timing of each group with the observer-reported participation values, we can see in which activities students were more engaged:


```{r pca-time-act}
# Looks at the start/end times of activities, and determines which activity was being performed for each observation (or NA)
lookupact <- function(x){
  act <- NA
  gr <- x['group']
  time <- as.POSIXct(x['timestamp'], tz="GMT")
  row <- tim_data[tim_data$group==gr & tim_data$start<=time & tim_data$end>=time,]
  if(nrow(row)>0) act <- as.character((row[,'activity'])[[1]])
  act
}

student_obs$activity <- as.factor(apply(student_obs, 1, lookupact))

aggr_part <- summarySE(student_obs, measurevar="participation",
                      groupvars=c("activity"))

ggplot(aggr_part[complete.cases(aggr_part),], aes(x=activity, y=participation, fill=activity)) + 
    geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=participation-ci, ymax=participation+ci),
                  width=.2,                    # Width of the error bars
                  position=position_dodge(.9))+theme_minimal()

```

We notice that overall participation was not very different across the different activities, *except* for activity II (Description & Selfie), the one that was considered considerably easier than the others. Although we do not have observer date about activity V (the wrap-up), the [teacher noted that groups did not participate in that activity either](https://drive.google.com/open?id=1Pio8UDC-iDvS6Vv1rR9do1WVS6BwFhwCikeWuuHBR_A).

We can do the same for our observer measures of student focus towards inside-outside the group, which were similar for all activities except activity III (the interviews). This was expected, since it forced the students to talk to quite a few people on the street.

```{r pca-time-act2}

aggr_focus <- summarySE(student_obs, measurevar="focusext",
                      groupvars=c("activity"))

ggplot(aggr_focus[complete.cases(aggr_focus),], aes(x=activity, y=focusext, fill=activity)) + 
    geom_bar(position=position_dodge(), stat="identity") +
    geom_errorbar(aes(ymin=focusext-ci, ymax=focusext+ci),
                  width=.2,                    # Width of the error bars
                  position=position_dodge(.9))+theme_minimal()

```


### Relationship with learning outcomes

We can also try to see if there was any relationship or trend between such observed values of participation and the learning outcomes assessed by the teacher:

```{r part-ass}

ag_part <- aggregate(participation~group+activity, data=student_obs, mean)
ass_part <- merge(melted_ass, ag_part)

ggplot(ass_part, aes(x=participation, y=assessment))+geom_point(size=3, alpha=0.3)+
  theme_minimal()+geom_smooth(method="lm")

#cor.test(ass_part$assessment, ass_part$participation) # p-value: 0.003362

ag_foc <- aggregate(focusext~group+activity, data=student_obs, mean)
ass_foc <- merge(melted_ass, ag_foc)
# 
# ggplot(ass_foc, aes(x=focusext, y=assessment))+geom_point(size=3, alpha=0.3)+
#   theme_minimal()+geom_smooth(method="lm")
# 
# #cor.test(ass_foc$assessment, ass_foc$focusext) # p-value: 0.94

```

There we can see that indeed there is a quite high correlation between the participation index as calculated by the observers, and the teacher assessment of the products of the activities (`r cor(ass_part$assessment, ass_part$participation)`). Hence, this variable is probably quite valuable to follow (even in real-time) in future project days. On the other hand, the "internal/external focus" index has virtually no correlation with the learning outcomes (`r cor(ass_foc$assessment, ass_foc$focusext)`).


### Engagement profiles (or clusters)

We can also try to see if there are different "engagement profiles" among different students. There seem to be about 5 different student profiles, according to their observed participation:

```{r kmeans}

students <- aggregate(cbind(disengaged,looking,talking,resources,external)~student.id,
                      data=student_obs[,4:9], FUN=mean)

# Determine number of clusters
wss <- (nrow(students)-1)*sum(apply(students,2,var))
for (i in 2:15){
  set.seed(12345)
  wss[i] <- sum(kmeans(students[,-1], 
  	centers=i)$withinss)
}
#plot(1:15, wss, type="b", xlab="Number of Clusters",
#  ylab="Within groups sum of squares")

# K-Means Cluster Analysis
set.seed(12345)
fit <- kmeans(students[,-1], 5) # 4 cluster solution
# get cluster means 

profiles <- aggregate(students[,-1],by=list(fit$cluster),FUN=mean)
profiles$Num.students <- table(fit$cluster)
names(profiles)[[1]] <- "Profile"
profiles$Profile <- as.factor(profiles$Profile)
levels(profiles$Profile) <- c("High.Part","Low.Part","Moderate.Part",
                              "Disengage.Look","Moderate.And.Talk")
kable(profiles)
# append cluster assignment
studentclust <- data.frame(students, fit$cluster)
names(studentclust)[[7]] <- "Profile"
studentclust$Profile <- as.factor(studentclust$Profile)
levels(studentclust$Profile) <- c("High.Part","Low.Part","Moderate.Part",
                              "Disengage.Look","Moderate.And.Talk")
kable(studentclust)
```

From these tables we can see the make-up of groups in terms of how many high- (or low-) participation students it has. For instance, we can compare Group 6 (which had lowest assessment scores -- made up of low and moderate participation students) vs. Group 2 (which had highest scores -- made up almost exclusively by students with high participation). Again, we see that there is a certain relationship between the participation/engagement of a group's participants and the assessment scores. Also, it is interesting to note that this profiles are able to single out the only international student that did not speak Estonian, and thus could not participate equally to the other students (Group 5 Student D).


### Equality (in engagement, in individual contribution)

A common feature in the study of collaborative learning and problem-solving is to look at equal participation (e.g., the absence of free-riders or otherwise passive members of the group). This equality can be measured using the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient), which goes from zero (perfect equality) to one (total inequality, one member doing all the work). We can use different data sources to see this equality: a) the individual contributions to the task self-reported by students; b) the active participation calculated from the observations above. We can thus ask ourselves how equal was the participation in each of the groups, from those two points of view:

```{r gini-grp}

ag_gini <- aggregate(contribution~group, data=fb1_data, FUN=ineq)

ag_part <- aggregate(participation~student.id+group, data = student_obs, FUN=mean)
ag_part$participation <- ag_part$participation-min(ag_part$participation) # We make the values all positive

ag_gini2 <- aggregate(participation~group, data=ag_part, FUN=ineq)

gini <- merge(ag_gini, ag_gini2)
names(gini) <- c("group", "student.reported", "observed")
ggplot(gini, aes(x=student.reported, y=observed, col=group))+geom_point(size=3)+xlim(0,1)+ylim(0,1)+ggtitle("Participation inequality (Gini coefficient)")+theme_minimal()
```

From the graph above we see that there is a certain correlation between the student-reported and observer-reported inequalities (corr=`r cor(gini$student.reported,gini$observed)`), and that Group 3 had much more equal participation than other groups (e.g., Group 1). 


```{r gini-act2}
aggini_actgrp <- aggregate(contribution~activity+group, data=fb1_data, FUN=ineq)
names(aggini_actgrp)[[3]] <- "Gini.selfreport"
ag_partact <- aggregate(participation~student.id+group+activity, data = student_obs, FUN=mean)
ag_partact$participation <- ag_partact$participation-min(ag_partact$participation) # We make the values all positive
aggini_actgrp2 <- aggregate(participation~activity+group, data=ag_partact, FUN=ineq)
names(aggini_actgrp2)[[3]] <- "Gini.observed"

aggini <- merge(aggini_actgrp,aggini_actgrp2, all=T)

aggini_ass <- merge(aggini, melted_ass, all=T)

#cor.test(aggini_ass$Gini.selfreport, aggini_ass$assessment, use="complete.obs") # cor=+0.19
#cor.test(aggini_ass$Gini.observed, aggini_ass$assessment, use="complete.obs") # cor=-0.48!

#ggplot(aggini_ass, aes(x=Gini.selfreport, y=Gini.observed))+
#  theme_minimal()+geom_point(size=4, alpha=0.3)+geom_smooth(method="lm", se=F)

```


However, if we look at the level of each activity, we see that this Gini coefficients (the observed and the self-reported), do not have that much to do (corr=`r cor(aggini_ass$Gini.selfreport, aggini_ass$Gini.observed, use="complete.obs")`).

We can also try to relate this (in)equality in participation (either self-reported or observed) with the learning outcomes: we find that there is a slight positive correlation between self-reported equality of participation and the learning outcomes (corr=`r cor(aggini_ass$Gini.selfreport, aggini_ass$assessment, use="complete.obs")`), while there is a strong *negative* correlation between the observed equality of participation and the learning outcomes (corr=`r cor(aggini_ass$Gini.observed, aggini_ass$assessment, use="complete.obs")`). This might indicate one or more things:

* that this equality of participation is not so important for achieving a group outcome in an activity (although such equality may be valuable in itself, to foster other skills like collaboration, not evaluated in the teacher assessment). 
* It might also indicate that different ways of participating are actually good for achieving a good final outcomes (i.e., if everyone is talking at the same time, it may be hard to achieve actual collaboration). Something to dig deeper in future analyses, maybe.

## Self-regulated learning (RQ3)

### Attention and instruction-following (functional reading)

As noted by the [teacher in her assessment of the activities](https://drive.google.com/open?id=1Pio8UDC-iDvS6Vv1rR9do1WVS6BwFhwCikeWuuHBR_A), multiple groups of students had problems understanding the instructions, and reflecting or seeing through figurative language. This may point towards the investigation of *functional reading* in the students, and how the activities could scaffold or develop this skill.

### Flow aspects

We can explore the relationship between difficulty of the activity and student satisfaction with that activity (as theory of flow indicates that engaging, flow-inducing activities tend to be challenging... but matching the skills of the learners):

```{r difficulty-satisfaction}
ggplot(fb1_data, aes(x=satisfaction, y=difficulty))+geom_jitter(size=3, width=0.3, height=0.3, alpha=0.5)+theme_minimal()+scale_y_continuous(breaks=seq(0,9,1), limits=c(0,9))+scale_x_continuous(breaks=seq(0,9,1), limits=c(0,9))+geom_smooth(method="lm",se=F)#+geom_smooth(col="grey", se=F)


```

As we can see, for all activities the trend is that, the more difficult the activities are perceived, students are less satisfied with their performance. This is, on the one hand, logical (nobody likes to do difficult things). On the other hand, the learning design should strive towards challenging tasks (as they can lead to more learning) that are nevertheless satisfying to complete.

An alternative way of looking at the same idea of how to approach the state of "flow" is to look at the relationship between preparedness and difficulty (in flow, the difficulty of the task matches the preparedness or skill of the subject), and see whether that matches the flow:

```{r difficulty-preparedness-satisfaction}
# Not very useful visualization
ggplot(fb1_data, aes(x=preparedness, y=difficulty, size=satisfaction))+geom_jitter(width=0.3, height=0.3, alpha=0.5)+theme_minimal()+scale_y_continuous(breaks=seq(0,9,1), limits=c(0,9))+scale_x_continuous(breaks=seq(0,9,1), limits=c(0,9))+scale_size(breaks=seq(0,9,1), limits=c(0,9))+#facet_wrap(~activity)+geom_smooth(method="lm",se=F)#
geom_smooth(method="lm",se=F)


```

Right now, we see the activities do not really follow the "ideal flow trend", which would be an upwards slope, with a lot of points in the top-right corner of the graph. Again, this suggests further work is needed to make activities that are both challenging but matching the students' skills (even if that is quite difficult, considering that students can be at very different levels of preparedness, even within a group).

### Student self-reflection on work and collaboration strategies

Looking at the students' own reflections of how they worked or how the distributed the tasks (in the [final questionnaire about the activities](https://drive.google.com/open?id=1R4EKJ1KYiuMqjT41p72XP4Lbsv_5JrDGfWRhQ_CSRNk)), we notice that responses are rather short and superficial, and in general indicate that there was not much thought put into how to collaborate or solve the problems (i.e., collaboration was rather free-form, with no defined roles or tasks).

This may (or may not) be something to explore in the future, for instance, whether assigning explicit roles or ways to collaborate, improves the collaboration process or the learning outcomes.


## Are there other analyses that would be interesting with the data available?

# Recommendations

Below are a few recommendations and proposals derived from the results shown above. **Please feel free to add your own conclusions, provide comments or opinions or alternative interpretations of the analysis results above!**

## Improving the learning scenario

* Use of a technological platform/environment to more effectively guide students, centralize evidence (student responses) and gather more data about the process (including real-time information on where they are!) --> should we use SmartZoos, Graasp or other environment?
* It looks like some of the activities were not done (students gave up on the poetry activity?), or had very low participation (e.g., the wrap-up done after lunch). Also, some activities took much longer than expected (e.g., the poem one). Should we adjust the tasks, or their difficulties, or expected durations?
* Also, right now the process of determining when each group is starting/ending what activity is quite messy, depending on multiple sources and manual work. Maybe the use of a centralized environment could also help us gather this information more easily.

## Improving the research process

* The use of a technological platform for all the exercises would provide us with a new data source (the actions of the students recorded by the platform). However, the way we suggest students to use it (e.g., use it as a group vs. individually) will lead to different kinds of data being generated (we may not have data about individuals' participation, unless we put some kind of observer with them).
* Right now the human effort in terms of observers is quite high, although the information they provide seems the most valuable in terms of understanding the level of engagement and group functioning. Can we find alternative ways of getting similar kinds of information (e.g., using sensors)?
* What other kinds of analysis do you think we should run on the available data?
* What other research questions would you be interested in exploring? Currently the collaboration/participation data seems quite rich and could provide interesting insights, but the self-regulated learning one is quite weak. Maybe we should focus on a more concrete aspect of self-regulated learning (e.g., attention, or functional reading)?
* Related to the previous one, maybe we should include more specialists/researchers in whatever main focuses we decide, to help us make the questions more concrete and to come up with better ways of gathering data (e.g., better questionnaires)

# References
